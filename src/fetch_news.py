#!/usr/bin/env python3
"""
Fetch AI/Tech news using RSS feeds and summarize with Claude.
"""

import anthropic
import feedparser
import json
import os
import requests
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
from concurrent.futures import ThreadPoolExecutor, as_completed

# Fallback RSS feeds (used when settings.json has no rss_feeds)
DEFAULT_RSS_FEEDS = [
    "https://techcrunch.com/feed/",
    "https://www.theverge.com/rss/index.xml",
    "https://feeds.arstechnica.com/arstechnica/technology-lab",
    "https://www.wired.com/feed/rss",
    "https://venturebeat.com/feed/",
    "https://www.technologyreview.com/feed/",
    "https://openai.com/blog/rss.xml",
    "https://blog.google/technology/ai/rss/",
    "https://ai.meta.com/blog/rss/",
    "https://www.anthropic.com/rss.xml",
    "https://hnrss.org/frontpage",
    "https://www.reddit.com/r/MachineLearning/.rss",
    "https://36kr.com/feed",
    "https://www.jiqizhixin.com/rss",
    "https://www.huxiu.com/rss/0.xml",
    "https://www.tmtpost.com/feed",
    "https://www.pingwest.com/feed",
    "https://www.geekpark.net/rss",
    "https://github.blog/feed/",
    "https://a16z.com/feed/",
]

def get_rss_feeds(settings: dict = None) -> list[str]:
    """Get RSS feed URLs from settings (enabled only), with fallback to defaults."""
    if settings is None:
        settings = load_settings()
    rss_feeds = settings.get("rss_feeds", [])
    if rss_feeds:
        return [f["url"] for f in rss_feeds if f.get("enabled", True)]
    return DEFAULT_RSS_FEEDS

# Default config path (relative to project root)
CONFIG_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config", "settings.json")

def load_settings() -> dict:
    """Load settings from config/settings.json.

    Backward-compatible: auto-migrates old formats to the new unified
    ``channels`` array.  Supports three legacy shapes:

    1. ``webhook_channels`` present (no ``channels``)  ‚Üí convert
    2. ``webhook_enabled`` present (no ``webhook_channels``, no ``channels``) ‚Üí convert
    3. Only top-level ``send_hour``/``send_minute``/``topic_mode``/``max_news_items`` ‚Üí convert
    """
    defaults = {
        "timezone": "Asia/Shanghai",
        "categories_order": ["‰∫ßÂìÅÂèëÂ∏É", "Â∑®Â§¥Âä®Âêë", "ÊäÄÊúØËøõÂ±ï", "Ë°å‰∏öËßÇÂØü", "ÊäïËûçËµÑ"],
        "filters": {
            "blacklist_keywords": [],
            "blacklist_sources": [],
            "whitelist_keywords": [],
            "whitelist_sources": []
        }
    }
    config_path = os.environ.get("SETTINGS_PATH", CONFIG_PATH)
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            settings = json.load(f)
        # Merge with defaults for any missing keys
        for k, v in defaults.items():
            settings.setdefault(k, v)

        # --- Backward-compatible migration to unified channels ---
        if "channels" not in settings:
            send_hour = settings.get("send_hour", 18)
            send_minute = settings.get("send_minute", 0)
            topic_mode = settings.get("topic_mode", "broad")
            max_items = settings.get("max_news_items", 10)

            channels = []

            # Email channel (always present)
            channels.append({
                "id": "email",
                "type": "email",
                "name": "ÈÇÆ‰ª∂",
                "enabled": True,
                "send_hour": send_hour,
                "send_minute": send_minute,
                "topic_mode": topic_mode,
                "max_news_items": max_items,
            })

            # Migrate webhook_channels or webhook_enabled
            if "webhook_channels" in settings:
                for ch in settings["webhook_channels"]:
                    channels.append({
                        "id": ch.get("id", "default"),
                        "type": "webhook",
                        "name": ch.get("name", "ÈªòËÆ§Áæ§"),
                        "enabled": ch.get("enabled", False),
                        "send_hour": send_hour,
                        "send_minute": send_minute,
                        "topic_mode": ch.get("topic_mode", topic_mode),
                        "max_news_items": max_items,
                        "webhook_url_base": ch.get("webhook_url_base", ""),
                    })
            elif settings.get("webhook_enabled", False):
                channels.append({
                    "id": "default",
                    "type": "webhook",
                    "name": "ÈªòËÆ§Áæ§",
                    "enabled": True,
                    "send_hour": send_hour,
                    "send_minute": send_minute,
                    "topic_mode": topic_mode,
                    "max_news_items": max_items,
                    "webhook_url_base": "",
                })

            settings["channels"] = channels

        return settings
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"  Warning: Could not load settings from {config_path}: {e}")
        return defaults

CATEGORY_ICONS = {
    # ËÅöÁÑ¶Ê®°ÂºèÁöÑ 3 ‰∏™ÂàÜÁ±ª
    "Êô∫ËÉΩÁ°¨‰ª∂": "ü•Ω",
    "AIÊäÄÊúØ‰∏é‰∫ßÂìÅ": "ü§ñ",
    "Â∑®Â§¥Âä®Âêë‰∏éË°å‰∏öËßÇÂØü": "üè¢",
    # Ê≥õ AI Ê®°ÂºèÁöÑ 5 ‰∏™ÂàÜÁ±ªÔºà‰øùÁïôÂÖºÂÆπÔºâ
    "‰∫ßÂìÅÂèëÂ∏É": "üöÄ",
    "Â∑®Â§¥Âä®Âêë": "üè¢",
    "ÊäÄÊúØËøõÂ±ï": "üî¨",
    "Ë°å‰∏öËßÇÂØü": "üìä",
    "ÊäïËûçËµÑ": "üí∞",
}

def get_categories(settings: dict = None) -> list[dict]:
    """Get ordered category list from settings."""
    if settings is None:
        settings = load_settings()
    order = settings.get("categories_order", list(CATEGORY_ICONS.keys()))
    return [{"name": name, "icon": CATEGORY_ICONS.get(name, "üì∞")} for name in order if name in CATEGORY_ICONS]

def get_time_window(settings: dict = None, manual: bool = False, channel: dict = None) -> tuple[str, str]:
    """Calculate the news time window.

    Args:
        settings: Configuration dict
        manual: If True, window ends at current time (for manual trigger)
                If False, window ends at scheduled send time (for auto trigger)
        channel: Optional channel dict ‚Äì uses its send_hour/send_minute if given.

    Returns:
        Tuple of (start_time, end_time) as formatted strings
    """
    if settings is None:
        settings = load_settings()

    if channel:
        send_hour = channel.get("send_hour", 18)
        send_minute = channel.get("send_minute", 0)
    else:
        # Fallback: use the first channel's time, or defaults
        channels = settings.get("channels", [])
        first = channels[0] if channels else {}
        send_hour = first.get("send_hour", settings.get("send_hour", 18))
        send_minute = first.get("send_minute", settings.get("send_minute", 0))

    tz_name = settings.get("timezone", "Asia/Shanghai")
    tz = ZoneInfo(tz_name)

    now = datetime.now(tz)

    if manual:
        # Manual trigger: window ends at current time
        end_time = now
    else:
        # Auto trigger: window ends at today's scheduled send time
        # Fetch always happens shortly before send_time, so use today's send_time
        end_time = now.replace(hour=send_hour, minute=send_minute, second=0, microsecond=0)

    start_time = end_time - timedelta(days=1)

    return (
        start_time.strftime("%Y-%m-%d %H:%M"),
        (end_time - timedelta(seconds=1)).strftime("%Y-%m-%d %H:%M")
    )

def get_cutoff_time(settings: dict = None, manual: bool = False, channel: dict = None) -> datetime:
    """Get the cutoff time for filtering articles.

    Args:
        settings: Configuration dict
        manual: If True, cutoff is 24h before now (for manual trigger)
                If False, cutoff is 24h before scheduled send time (for auto trigger)
        channel: Optional channel dict ‚Äì uses its send_hour/send_minute if given.
    """
    if settings is None:
        settings = load_settings()
    tz_name = settings.get("timezone", "Asia/Shanghai")
    tz = ZoneInfo(tz_name)

    if channel:
        send_hour = channel.get("send_hour", 18)
        send_minute = channel.get("send_minute", 0)
    else:
        channels = settings.get("channels", [])
        first = channels[0] if channels else {}
        send_hour = first.get("send_hour", settings.get("send_hour", 18))
        send_minute = first.get("send_minute", settings.get("send_minute", 0))

    now = datetime.now(tz)

    if manual:
        # Manual trigger: 24h before now
        return (now - timedelta(days=1)).replace(tzinfo=None)
    else:
        # Auto trigger: 24h before today's scheduled send time
        today_send = now.replace(hour=send_hour, minute=send_minute, second=0, microsecond=0)
        return (today_send - timedelta(days=1)).replace(tzinfo=None)

def parse_feed(feed_url: str, cutoff: datetime = None) -> list[dict]:
    """Parse a single RSS feed and return recent articles."""
    articles = []
    if cutoff is None:
        cutoff = datetime.now() - timedelta(hours=24)

    try:
        # Use requests to fetch content first (handles SSL better than feedparser's urllib)
        try:
            resp = requests.get(feed_url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            resp.raise_for_status()
            feed = feedparser.parse(resp.content)
        except requests.RequestException:
            return []
        source_name = feed.feed.get("title", feed_url)

        for entry in feed.entries[:20]:  # Limit entries per feed
            # Parse published time
            published = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                published = datetime(*entry.updated_parsed[:6])

            # Skip if too old or no date
            if published and published < cutoff:
                continue

            articles.append({
                "title": entry.get("title", ""),
                "description": entry.get("summary", entry.get("description", ""))[:500],
                "source": source_name,
                "feed_url": feed_url,
                "url": entry.get("link", ""),
                "published": published.isoformat() if published else ""
            })
    except Exception as e:
        print(f"  Warning: Failed to parse {feed_url}: {e}")

    return articles

def fetch_raw_news(cutoff: datetime = None, settings: dict = None, max_per_source: int = 3, hardware_unlimited: bool = False) -> list[dict]:
    """Fetch raw news from multiple RSS feeds in parallel.

    Args:
        cutoff: Only include articles published after this time
        settings: Settings dict
        max_per_source: Maximum articles to keep per source (ensures diversity)
        hardware_unlimited: If True, smart hardware sources are not limited (only for focused mode)
    """
    if settings is None:
        settings = load_settings()

    all_articles = []
    feed_urls = get_rss_feeds(settings)
    print(f"  - Using {len(feed_urls)} RSS feeds")

    # Ëé∑ÂèñÊô∫ËÉΩÁ°¨‰ª∂Ê∫êÁöÑ URL ÂàóË°®Ôºà‰ªÖËÅöÁÑ¶Ê®°Âºè‰∏ã‰∏çÂèóÈôêÂà∂Ôºâ
    hardware_urls = set()
    if hardware_unlimited:
        rss_feeds = settings.get("rss_feeds", [])
        for feed in rss_feeds:
            if feed.get("group") == "Êô∫ËÉΩÁ°¨‰ª∂" and feed.get("enabled", True):
                hardware_urls.add(feed.get("url", ""))
        print(f"  - Smart hardware sources (no limit): {len(hardware_urls)} feeds")

    # Collect articles grouped by source
    articles_by_source = {}

    failed_feeds = []
    timeout_feeds = []
    empty_feeds = []

    import time
    rss_start = time.time()

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(parse_feed, url, cutoff): url for url in feed_urls}

        for future in as_completed(futures):
            url = futures[future]
            try:
                articles = future.result()
                if not articles:
                    empty_feeds.append(url)
                for article in articles:
                    source = article.get("source", "unknown")
                    if source not in articles_by_source:
                        articles_by_source[source] = []
                    articles_by_source[source].append(article)
            except Exception as e:
                err_str = str(e).lower()
                if 'timeout' in err_str or 'timed out' in err_str:
                    timeout_feeds.append(url)
                else:
                    failed_feeds.append((url, str(e)))

    rss_elapsed = time.time() - rss_start
    print(f"  - RSS ÊäìÂèñËÄóÊó∂: {rss_elapsed:.1f}s")
    print(f"  - ÊàêÂäü: {len(feed_urls) - len(failed_feeds) - len(timeout_feeds) - len(empty_feeds)}, Á©∫: {len(empty_feeds)}, Ë∂ÖÊó∂: {len(timeout_feeds)}, Â§±Ë¥•: {len(failed_feeds)}")
    if timeout_feeds:
        print(f"  - Ë∂ÖÊó∂Ê∫ê: {[u.split('/')[-1][:25] for u in timeout_feeds[:5]]}")
    if failed_feeds:
        print(f"  - Â§±Ë¥•Ê∫ê: {[f[0].split('/')[-1][:25] for f in failed_feeds[:5]]}")

    # Limit articles per source and merge
    # ËÅöÁÑ¶Ê®°ÂºèÔºöÊô∫ËÉΩÁ°¨‰ª∂Ê∫ê‰∏çÂèóÈôêÂà∂ÔºõÊ≥õAIÊ®°ÂºèÔºöÊâÄÊúâÊ∫êÂùáÂèóÈôêÂà∂
    hardware_article_count = 0
    for source, articles in articles_by_source.items():
        # Sort by published time within source
        articles.sort(key=lambda x: x.get("published", ""), reverse=True)

        # Ê£ÄÊü•ÊòØÂê¶ÊòØÊô∫ËÉΩÁ°¨‰ª∂Ê∫êÔºàÈÄöËøá feed_url Á≤æÁ°ÆÂåπÈÖçÔºâ
        is_hardware = hardware_unlimited and any(
            a.get("feed_url", "") in hardware_urls for a in articles
        ) if hardware_urls else False

        if is_hardware:
            # Êô∫ËÉΩÁ°¨‰ª∂Ê∫êÔºöÂÖ®ÈÉ®‰øùÁïôÔºà‰ªÖËÅöÁÑ¶Ê®°ÂºèÔºâ
            all_articles.extend(articles)
            hardware_article_count += len(articles)
        else:
            # ÂÖ∂‰ªñÊ∫êÔºöÈôêÂà∂Êï∞Èáè
            all_articles.extend(articles[:max_per_source])

    # Sort all by published time (newest first)
    all_articles.sort(key=lambda x: x.get("published", ""), reverse=True)

    print(f"  - Sources with articles: {len(articles_by_source)}")
    if hardware_unlimited:
        print(f"  - Smart hardware articles (unlimited): {hardware_article_count}")
    # Show top sources by article count
    source_counts = [(src, len(arts)) for src, arts in articles_by_source.items()]
    source_counts.sort(key=lambda x: -x[1])
    print(f"  - Top sources: {source_counts[:10]}")

    return all_articles

def apply_filters(articles: list[dict], settings: dict = None) -> list[dict]:
    """Apply blacklist/whitelist filters from settings to articles."""
    if settings is None:
        settings = load_settings()
    filters = settings.get("filters", {})
    blacklist_kw = [kw.lower() for kw in filters.get("blacklist_keywords", [])]
    blacklist_src = [src.lower() for src in filters.get("blacklist_sources", [])]
    whitelist_kw = [kw.lower() for kw in filters.get("whitelist_keywords", [])]
    whitelist_src = [src.lower() for src in filters.get("whitelist_sources", [])]

    if not any([blacklist_kw, blacklist_src, whitelist_kw, whitelist_src]):
        return articles

    filtered = []
    for article in articles:
        title = (article.get("title", "") or "").lower()
        desc = (article.get("description", "") or "").lower()
        source = (article.get("source", "") or "").lower()
        text = title + " " + desc

        # Blacklist: skip if matches
        if any(kw in text for kw in blacklist_kw):
            continue
        if any(src in source for src in blacklist_src):
            continue

        filtered.append(article)

    # Whitelist: boost matching articles to the front
    if whitelist_kw or whitelist_src:
        boosted = []
        normal = []
        for article in filtered:
            title = (article.get("title", "") or "").lower()
            desc = (article.get("description", "") or "").lower()
            source = (article.get("source", "") or "").lower()
            text = title + " " + desc
            if any(kw in text for kw in whitelist_kw) or any(src in source for src in whitelist_src):
                boosted.append(article)
            else:
                normal.append(article)
        filtered = boosted + normal

    return filtered

def get_prompt_for_mode(mode: str, articles_text: str, max_items: int, category_names: str, category_json_example: str, icon_mapping: str, custom_prompt: str = None, paywalled_sources: str = "") -> str:
    """Generate the Claude prompt based on topic mode or custom prompt.

    If custom_prompt is provided, it will be used directly with variable substitution:
    - {articles_text} - The news articles text
    - {max_items} - Maximum number of news items
    - {category_names} - Category names joined by „ÄÅ
    - {category_json_example} - Example JSON structure
    - {icon_mapping} - Icon mapping string
    - {paywalled_sources} - Comma-separated list of paywalled source names
    """

    if custom_prompt:
        # Use custom prompt with variable substitution
        try:
            return custom_prompt.format(
                articles_text=articles_text,
                max_items=max_items,
                category_names=category_names,
                category_json_example=category_json_example,
                icon_mapping=icon_mapping,
                paywalled_sources=paywalled_sources
            )
        except KeyError as e:
            print(f"  Warning: Custom prompt has invalid variable {e}, falling back to mode-based prompt")

    if mode == "focused":
        # ËÅöÁÑ¶Ê®°ÂºèÔºöÊãÜÊàê‰∏§‰∏™Áã¨Á´ã promptÔºåÂàÜÂà´Ë∞ÉÁî®ÂêéÂêàÂπ∂
        # ËøîÂõû NoneÔºåÁî± summarize_news_with_claude Â§ÑÁêÜÊãÜÂàÜË∞ÉÁî®
        return None

    if mode == "focused_hardware":
        return f"""‰ª•‰∏ãÊòØÊúÄËøë24Â∞èÊó∂ÂÜÖ‰ªéÂ§ö‰∏™Êù•Ê∫êÊäìÂèñÁöÑÊñ∞ÈóªÂàóË°®„ÄÇËØ∑‰ªé‰∏≠Á≠õÈÄâÂá∫‰∏é**AIÊô∫ËÉΩÁ°¨‰ª∂ËÆæÂ§á**Áõ∏ÂÖ≥ÁöÑÊñ∞Èóª„ÄÇ

**Â±û‰∫éÊô∫ËÉΩÁ°¨‰ª∂ÁöÑËåÉÂõ¥**ÔºàÈù¢ÂêëÊ∂àË¥πËÄÖÊàñË°å‰∏öÁöÑAIËÆæÂ§áÔºâÔºö
- AR/VR/MR/XR Â§¥Êòæ„ÄÅÊô∫ËÉΩÁúºÈïúÔºàMeta Ray-Ban„ÄÅApple Vision Pro„ÄÅXREAL„ÄÅRokid Á≠âÔºâ
- AI Á©øÊà¥ËÆæÂ§áÔºàAI ÊâãË°®„ÄÅAI ÊàíÊåá„ÄÅAI ËÄ≥Êú∫Á≠âÔºâ
- Êú∫Âô®‰∫∫Ôºà‰∫∫ÂΩ¢Êú∫Âô®‰∫∫„ÄÅÊúçÂä°Êú∫Âô®‰∫∫„ÄÅÂ∑•‰∏öÊú∫Âô®‰∫∫Ôºâ
- AI ÁªàÁ´ØËÆæÂ§áÔºàAI ÊâãÊú∫„ÄÅAI PC Á≠âÂÖ∑‰ΩìËÆæÂ§á‰∫ßÂìÅÔºâ

**‰∏çÂ±û‰∫éÊô∫ËÉΩÁ°¨‰ª∂**ÔºàÂøÖÈ°ªÊéíÈô§ÔºåËøô‰∫õÂ∫îÊîæÂú®Âà´ÁöÑÁ±ªÂà´ÔºâÔºö
- AI ËäØÁâá„ÄÅAI Âü∫Á°ÄËÆæÊñΩ„ÄÅÊï∞ÊçÆ‰∏≠ÂøÉÊäïËµÑÔºà‚Üí Â±û‰∫éAIÊäÄÊúØ/Ë°å‰∏öÔºâ
- Âç´Êòü„ÄÅËà™Â§©Âô®„ÄÅÂ§™Á©∫ËÆæÂ§áÔºà‚Üí Â±û‰∫éË°å‰∏öËßÇÂØüÔºâ
- ‰º†ÁªüÁîµËÑë„ÄÅÊ∏∏Êàè‰∏ªÊú∫ÔºàPlayStation„ÄÅXbox„ÄÅSwitchÔºâ
- ÊôÆÈÄöÊ∂àË¥πÁîµÂ≠êÔºàÁîµËßÜ„ÄÅÈü≥ÁÆ±„ÄÅÁõ∏Êú∫Á≠âÔºâ
- Á∫ØËΩØ‰ª∂‰∫ßÂìÅ„ÄÅ‰∫íËÅîÁΩëÊúçÂä°

**Êï∞ÈáèË¶ÅÊ±Ç**ÔºöÈÄâ 5-7 Êù°Ôºå‰∏çÂ§ö‰∏çÂ∞ë„ÄÇ

**Á≠õÈÄâË¶ÅÊ±Ç**Ôºö
- ÂéªÈáçÔºöÁõ∏Âêå‰∫ã‰ª∂Âè™‰øùÁïôÊúÄÊùÉÂ®ÅÊù•Ê∫ê
- ÊåâÈáçË¶ÅÊÄßÊéíÂ∫è

**Êù•Ê∫êÊùÉÂ®ÅÊÄß‰ºòÂÖà**Ôºö
- Â¶ÇÊûúÊüêÊù°Êñ∞ÈóªÊù•Ëá™Â∞è‰ºóÊù•Ê∫êÔºàÂ¶Ç UploadVR, 93913, VRÈôÄËû∫ Á≠âÔºâÔºåÊ£ÄÊü•ÊòØÂê¶ÊúâÊùÉÂ®ÅÊù•Ê∫êÔºàThe Verge, TechCrunch, Wired Á≠âÔºâÊä•ÈÅì‰∫ÜÂÆåÂÖ®Áõ∏ÂêåÁöÑ‰∫ã‰ª∂
- Âè™ÊúâÁ°ÆÂÆöÊòØÂêå‰∏Ä‰∫ã‰ª∂Êó∂ÔºåÊâçÊõøÊç¢‰∏∫ÊùÉÂ®ÅÊù•Ê∫ê URL
- ‚ö†Ô∏è ‰ΩøÁî®Êüê‰∏™Êù•Ê∫êÁöÑ URL Êó∂ÔºåÊëòË¶ÅÂøÖÈ°ªÂáÜÁ°ÆÂèçÊò†ËØ• URL ÊñáÁ´†ÁöÑÂÜÖÂÆπ

**‰ªòË¥πÂ¢ôÂ§ÑÁêÜ**Ôºö
‰ªòË¥πÂ¢ôÂ™í‰ΩìÔºö{paywalled_sources}
- Â¶ÇÊúâÂÖçË¥πÊõø‰ª£Ê∫êÊä•ÈÅìÁõ∏Âêå‰∫ã‰ª∂Ôºå‰ΩøÁî®ÂÖçË¥πÊ∫ê URL

**ËæìÂá∫Ë¶ÅÊ±Ç**Ôºö
- ‰∏∫ÊØèÊù°Êñ∞ÈóªÂÜô‰∏Ä‰∏™ÁÆÄÁü≠ÁöÑ‰∏≠ÊñáÊëòË¶ÅÔºà1-2Âè•ËØùÔºâ
- ‰∏∫ÊØèÊù°Êñ∞ÈóªÊ∑ªÂä†‰∏ÄÂè• commentÔºåÂøÖÈ°ªÊòØ‰∏Ä‰∏™ÂêØÂèëÊÄùËÄÉÁöÑÈóÆÈ¢òÔºà‰ª•ÔºüÁªìÂ∞æÔºâ

Êñ∞ÈóªÂàóË°®Ôºö
{articles_text}

ËØ∑‰ª• JSON Ê†ºÂºèËøîÂõûÔºåÁªìÊûÑÂ¶Ç‰∏ãÔºö
{{
  "news": [
    {{
      "title": "‰∏≠ÊñáÊ†áÈ¢ò",
      "summary": "1-2Âè•‰∏≠ÊñáÊëòË¶Å",
      "comment": "‰∏Ä‰∏™ÂêØÂèëÊÄùËÄÉÁöÑÈóÆÈ¢òÔºü",
      "source": "Êù•Ê∫ê",
      "url": "ÈìæÊé•"
    }}
  ]
}}

Ê≥®ÊÑèÔºö
- Ê†áÈ¢òÂøÖÈ°ªÁøªËØë‰∏∫‰∏≠ÊñáÔºåËã±ÊñáÊ†áÈ¢ò‰∏ÄÂæãÁøªËØë
- Âè™ËøîÂõûÂêàÊ≥ïÁöÑ JSONÔºå‰∏çË¶ÅÂÖ∂‰ªñÊñáÂ≠ó
- Á°Æ‰øùÊâÄÊúâÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑÂèåÂºïÂè∑Áî®ÂçïÂºïÂè∑ÊõøÊç¢"""

    if mode == "focused_ai_industry":
        return f"""‰ª•‰∏ãÊòØÊúÄËøë24Â∞èÊó∂ÂÜÖ‰ªéÂ§ö‰∏™Êù•Ê∫êÊäìÂèñÁöÑÊñ∞ÈóªÂàóË°®„ÄÇËØ∑‰ªé‰∏≠Á≠õÈÄâÂá∫‰∏é‰ª•‰∏ã‰∏§‰∏™ÂàÜÁ±ªÁõ∏ÂÖ≥ÁöÑÊñ∞Èóª„ÄÇ

**ÂàÜÁ±ª 1Ôºöü§ñ AIÊäÄÊúØ‰∏é‰∫ßÂìÅ**
- Ê®°ÂûãËÉΩÂäõÊèêÂçáÔºöÊé®ÁêÜËÉΩÂäõ„ÄÅÂ§öÊ®°ÊÄÅ„ÄÅÈïø‰∏ä‰∏ãÊñá„ÄÅAgent ËÉΩÂäõÁ≠â
- Êñ∞‰∫ßÂìÅÂΩ¢ÊÄÅÔºöAI Agent„ÄÅAI ÁºñÁ®ãÂ∑•ÂÖ∑„ÄÅAI Âàõ‰ΩúÂ∑•ÂÖ∑„ÄÅAI Â∫îÁî®
- Êñ∞ËåÉÂºèÔºöÁ´Ø‰æß AI„ÄÅÂºÄÊ∫êÊ®°Âûã„ÄÅAI Âü∫Á°ÄËÆæÊñΩ„ÄÅËÆ≠ÁªÉ/Êé®ÁêÜ‰ºòÂåñ

**ÂàÜÁ±ª 2Ôºöüè¢ Â∑®Â§¥Âä®Âêë‰∏éË°å‰∏öËßÇÂØü**
- ÁßëÊäÄÂ∑®Â§¥ÁöÑ**AIÁõ∏ÂÖ≥**ÊàòÁï•Â∏ÉÂ±Ä„ÄÅÂπ∂Ë¥≠Êî∂Ë¥≠
  - ÂõΩÂÜÖÂ§ßÂéÇÔºàÂ≠óËäÇË∑≥Âä®/Ë±ÜÂåÖ„ÄÅÈòøÈáå/ÈÄö‰πâÂçÉÈóÆ„ÄÅËÖæËÆØ„ÄÅÁôæÂ∫¶/ÊñáÂøÉ„ÄÅÂçé‰∏∫„ÄÅÂ∞èÁ±≥„ÄÅÁæéÂõ¢„ÄÅ‰∫¨‰∏úÁ≠âÔºâÁöÑ AI Âä®ÊÄÅË¶ÅÈáçÁÇπÂÖ≥Ê≥®
  - Êµ∑Â§ñÂ∑®Â§¥ÔºàOpenAI„ÄÅGoogle„ÄÅMeta„ÄÅMicrosoft„ÄÅApple„ÄÅAmazon Á≠âÔºâ
- AI Ë°å‰∏öË∂ãÂäø„ÄÅAI ÊîøÁ≠ñÊ≥ïËßÑ
- AI È¢ÜÂüüÈáçÂ§ßÊäïËûçËµÑ‰∫ã‰ª∂ÔºàÂåÖÊã¨AIËäØÁâá„ÄÅAIÂü∫Á°ÄËÆæÊñΩÊäïËµÑÔºâ
- AI Áõ∏ÂÖ≥ÈáçË¶Å‰∫∫‰∫ãÂèòÂä®

‚ö†Ô∏è **‰∏≠Â§ñÊñ∞ÈóªÂπ≥Ë°°**ÔºöÂ¶ÇÊúâÂõΩÂÜÖÂ§ßÂéÇÁõ∏ÂÖ≥ÁöÑ AI Êñ∞ÈóªÔºåËá≥Â∞ëÈÄâÂÖ• 1 Êù°„ÄÇ‰∏çË¶ÅÂÖ®ÊòØÊµ∑Â§ñÊñ∞Èóª„ÄÇ

‚ö†Ô∏è **ÊâÄÊúâÊñ∞ÈóªÂøÖÈ°ª‰∏éAI/‰∫∫Â∑•Êô∫ËÉΩÁõ¥Êé•Áõ∏ÂÖ≥**„ÄÇ‰ª•‰∏ã‰∏çÊî∂ÂΩïÔºö
- ‰∏éAIÊó†ÂÖ≥ÁöÑÁßëÊäÄÊñ∞ÈóªÔºà‰º†ÁªüÂ™í‰Ωì‰∫∫‰∫ã„ÄÅÈùûAIÂÖ¨Âè∏Ë£ÅÂëò„ÄÅÂä†ÂØÜË¥ßÂ∏ÅÁ≠âÔºâ
- Á∫ØÂïÜ‰∏ö/ÈáëËûçÊñ∞ÈóªÔºàÈô§ÈùûÁõ¥Êé•Ê∂âÂèäAIÊäïËµÑÔºâ

**Á≠õÈÄâË¶ÅÊ±Ç**Ôºö
- ÊéíÈô§ÊâÄÊúâÊô∫ËÉΩÁ°¨‰ª∂ËÆæÂ§áÊñ∞ÈóªÔºàAR/VRÂ§¥Êòæ„ÄÅÊô∫ËÉΩÁúºÈïú„ÄÅÊú∫Âô®‰∫∫Á≠âÂÖ∑‰ΩìËÆæÂ§áÔºâ
- AIÊäÄÊúØ‰∏é‰∫ßÂìÅÔºöËá≥Â∞ëÈÄâ 2 Êù°
- Â∑®Â§¥Âä®Âêë‰∏éË°å‰∏öËßÇÂØüÔºöËá≥Â∞ëÈÄâ 1 Êù°
- ‰∏§‰∏™ÂàÜÁ±ªÂêàËÆ°ÊúÄÂ§ö {max_items} Êù°
- ÂéªÈáçÔºöÁõ∏Âêå‰∫ã‰ª∂Âè™‰øùÁïôÊúÄÊùÉÂ®ÅÊù•Ê∫ê
- ÊåâÈáçË¶ÅÊÄßÊéíÂ∫è

**Êù•Ê∫êÊùÉÂ®ÅÊÄß‰ºòÂÖà**Ôºö
- ‰∏≠ÊñáÊùÉÂ®ÅÊù•Ê∫êÔºö36Ê∞™„ÄÅÊú∫Âô®‰πãÂøÉ„ÄÅÈáèÂ≠ê‰Ωç„ÄÅËôéÂóÖ„ÄÅÈíõÂ™í‰Ωì„ÄÅÊôöÁÇπLatePost„ÄÅFounder Park
- Ëã±ÊñáÊùÉÂ®ÅÊù•Ê∫êÔºöThe Verge, TechCrunch, Reuters, Bloomberg, Wired
- ‚ö†Ô∏è ‰ΩøÁî®Êüê‰∏™Êù•Ê∫êÁöÑ URL Êó∂ÔºåÊëòË¶ÅÂøÖÈ°ªÂáÜÁ°ÆÂèçÊò†ËØ• URL ÊñáÁ´†ÁöÑÂÜÖÂÆπ

**‰ªòË¥πÂ¢ôÂ§ÑÁêÜ**Ôºö
‰ªòË¥πÂ¢ôÂ™í‰ΩìÔºö{paywalled_sources}
- Â¶ÇÊúâÂÖçË¥πÊõø‰ª£Ê∫êÊä•ÈÅìÁõ∏Âêå‰∫ã‰ª∂Ôºå‰ΩøÁî®ÂÖçË¥πÊ∫ê URL

**ËæìÂá∫Ë¶ÅÊ±Ç**Ôºö
- ‰∏∫ÊØèÊù°Êñ∞ÈóªÂÜô‰∏Ä‰∏™ÁÆÄÁü≠ÁöÑ‰∏≠ÊñáÊëòË¶ÅÔºà1-2Âè•ËØùÔºâ
- ‰∏∫ÊØèÊù°Êñ∞ÈóªÊ∑ªÂä†‰∏ÄÂè• commentÔºåÂøÖÈ°ªÊòØ‰∏Ä‰∏™ÂêØÂèëÊÄùËÄÉÁöÑÈóÆÈ¢òÔºà‰ª•ÔºüÁªìÂ∞æÔºâ

Êñ∞ÈóªÂàóË°®Ôºö
{articles_text}

ËØ∑‰ª• JSON Ê†ºÂºèËøîÂõûÔºåÁªìÊûÑÂ¶Ç‰∏ãÔºö
{{
  "categories": [
    {{
      "name": "AIÊäÄÊúØ‰∏é‰∫ßÂìÅ",
      "icon": "ü§ñ",
      "news": [...]
    }},
    {{
      "name": "Â∑®Â§¥Âä®Âêë‰∏éË°å‰∏öËßÇÂØü",
      "icon": "üè¢",
      "news": [...]
    }}
  ]
}}

ÊØèÊù° news ÁöÑÁªìÊûÑÔºö
{{
  "title": "‰∏≠ÊñáÊ†áÈ¢ò",
  "summary": "1-2Âè•‰∏≠ÊñáÊëòË¶Å",
  "comment": "‰∏Ä‰∏™ÂêØÂèëÊÄùËÄÉÁöÑÈóÆÈ¢òÔºü",
  "source": "Êù•Ê∫ê",
  "url": "ÈìæÊé•"
}}

Ê≥®ÊÑèÔºö
- ‰∏§‰∏™ÂàÜÁ±ªÈÉΩÂøÖÈ°ªÊúâÂÜÖÂÆπ
- Ê†áÈ¢òÂøÖÈ°ªÁøªËØë‰∏∫‰∏≠ÊñáÔºåËã±ÊñáÊ†áÈ¢ò‰∏ÄÂæãÁøªËØë
- Âè™ËøîÂõûÂêàÊ≥ïÁöÑ JSONÔºå‰∏çË¶ÅÂÖ∂‰ªñÊñáÂ≠ó
- Á°Æ‰øùÊâÄÊúâÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑÂèåÂºïÂè∑Áî®ÂçïÂºïÂè∑ÊõøÊç¢"""

    else:
        # Ê≥õ AI Ê®°ÂºèÔºàÈªòËÆ§Ôºâ
        return f"""‰ª•‰∏ãÊòØÊúÄËøë24Â∞èÊó∂ÂÜÖ‰ªéÂ§ö‰∏™Êù•Ê∫êÊäìÂèñÁöÑÊñ∞ÈóªÂàóË°®„ÄÇËØ∑Â∏ÆÊàëÔºö

1. **‰∏•Ê†ºÁ≠õÈÄâ**ÔºöÂè™‰øùÁïô‰∏é AIÔºà‰∫∫Â∑•Êô∫ËÉΩÔºâÁõ¥Êé•Áõ∏ÂÖ≥ÁöÑÊñ∞Èóª
   - ÂøÖÈ°ªÂåÖÂê´ÁöÑÔºöAI Ê®°ÂûãÂèëÂ∏É/Êõ¥Êñ∞„ÄÅAI ÂÖ¨Âè∏Âä®ÊÄÅ„ÄÅAI ËûçËµÑ„ÄÅAI ‰∫ßÂìÅ„ÄÅAI ÊîøÁ≠ñÊ≥ïËßÑ„ÄÅAI Â∫îÁî®ËêΩÂú∞„ÄÅÂ§ßÊ®°Âûã„ÄÅÊú∫Âô®Â≠¶‰π†„ÄÅÊ∑±Â∫¶Â≠¶‰π†„ÄÅAIGC„ÄÅAGI„ÄÅÊú∫Âô®‰∫∫„ÄÅËá™Âä®È©æÈ©∂Á≠â
   - ÂøÖÈ°ªÊéíÈô§ÁöÑÔºö‰∏é AI Êó†ÂÖ≥ÁöÑÊôÆÈÄöÁßëÊäÄÊñ∞ÈóªÔºàÂ¶ÇÊâãÊú∫ÂèëÂ∏É„ÄÅÊ∏∏Êàè„ÄÅÁîµÂïÜ‰øÉÈîÄ„ÄÅÁ§æ‰∫§Â™í‰ΩìÂÖ´Âç¶„ÄÅÁ∫ØÁ°¨‰ª∂ËØÑÊµãÁ≠âÔºâ
   - ËæπÁïåÊÉÖÂÜµÔºöÂ¶ÇÊûú‰∏ÄÊù°Êñ∞Èóª‰∏ªË¶ÅËÆ≤ÊüêÁßëÊäÄÂÖ¨Âè∏‰ΩÜÊ†∏ÂøÉÂÜÖÂÆπ‰∏é AI Êó†ÂÖ≥ÔºåÂ∫îÊéíÈô§
2. ÂéªÈáçÔºöÁõ∏Âêå‰∫ã‰ª∂ÁöÑÂ§öÁØáÊä•ÈÅìÂè™‰øùÁïô‰∏ÄÊù°Ôºà‰øùÁïôÊúÄÊùÉÂ®ÅÊù•Ê∫êÔºâ
3. **‰∏≠Â§ñÂπ≥Ë°°**ÔºöÂ¶ÇÊúâÂõΩÂÜÖÂ§ßÂéÇÔºàÂ≠óËäÇË∑≥Âä®/Ë±ÜÂåÖ„ÄÅÈòøÈáå/ÈÄö‰πâÂçÉÈóÆ„ÄÅËÖæËÆØ„ÄÅÁôæÂ∫¶/ÊñáÂøÉ„ÄÅÂçé‰∏∫„ÄÅÂ∞èÁ±≥Á≠âÔºâÁöÑ AI Áõ∏ÂÖ≥Êñ∞ÈóªÔºåËá≥Â∞ëÈÄâÂÖ• 1 Êù°Ôºå‰∏çË¶ÅÂÖ®ÊòØÊµ∑Â§ñÊñ∞Èóª
4. ÊåâÈáçË¶ÅÊÄßÊéíÂ∫èÔºàÂÖ®ÁêÉÂΩ±Âìç > Ë°å‰∏öÂΩ±Âìç > Âå∫ÂüüÂΩ±ÂìçÔºâ
4. ‰∏∫ÊØèÊù°Êñ∞ÈóªÂÜô‰∏Ä‰∏™ÁÆÄÁü≠ÁöÑ‰∏≠ÊñáÊëòË¶ÅÔºà1-2Âè•ËØùÔºâ
5. **ÈáçË¶Å**Ôºö‰∏∫ÊØèÊù°Êñ∞ÈóªÊ∑ªÂä†‰∏ÄÂè• commentÔºåÂøÖÈ°ªÊòØ‰∏Ä‰∏™ÂêØÂèëÊÄùËÄÉÁöÑÈóÆÈ¢òÔºà‰ª•ÔºüÁªìÂ∞æÔºâÔºåÂºïÂØºËØªËÄÖÊ∑±ÂÖ•ÊÄùËÄÉËøôÊù°Êñ∞ÈóªÁöÑÊÑè‰πâ„ÄÅÂΩ±ÂìçÊàñÊú™Êù•ÂèØËÉΩÊÄß
6. Â∞ÜÊñ∞ÈóªÊåâ‰ª•‰∏ãÁ±ªÂà´ÂàÜÁªÑÔºö{category_names}
   - ÊØèÊù°Êñ∞ÈóªÂè™ÂΩíÂÖ•‰∏Ä‰∏™ÊúÄÂåπÈÖçÁöÑÁ±ªÂà´
   - Ê≤°ÊúâÂØπÂ∫îÊñ∞ÈóªÁöÑÁ±ªÂà´‰∏çË¶ÅËæìÂá∫

**ÈáçË¶Å**ÔºöÊÄªÂÖ±ÊúÄÂ§öÈÄâ {max_items} Êù°ÊúÄÂÄºÂæóÁúãÁöÑÊñ∞ÈóªÔºà‰∏çÊòØÊØè‰∏™ÂàÜÁ±ª {max_items} Êù°ÔºâÔºåÂú®Ëøô‰∫õÊñ∞Èóª‰∏≠ÂΩíÁ±ªÊéíÂàó„ÄÇ
ÊëòË¶ÅÂíåÊ†áÈ¢ò‰∏≠‰∏çË¶Å‰ΩøÁî®ÂèåÂºïÂè∑ÔºåÁî®ÂçïÂºïÂè∑ÊàñÂÖ∂‰ªñÊ†áÁÇπ‰ª£Êõø„ÄÇ

Êñ∞ÈóªÂàóË°®Ôºö
{articles_text}

ËØ∑‰ª• JSON Ê†ºÂºèËøîÂõûÔºåÁªìÊûÑÂ¶Ç‰∏ãÔºö
{{
  "categories": {category_json_example}
}}

Ê≥®ÊÑèÔºö
- Âè™ËøîÂõûÊúâÊñ∞ÈóªÁöÑÁ±ªÂà´
- icon ÂøÖÈ°ª‰∏éÁ±ªÂà´ÂØπÂ∫îÔºà{icon_mapping}Ôºâ
- ÊØèÊù° news ÂøÖÈ°ªÂåÖÂê´ comment Â≠óÊÆµÔºàÂêØÂèëÊÄùËÄÉÁöÑÈóÆÂè•Ôºå‰ª•ÔºüÁªìÂ∞æÔºâ
- Âè™ËøîÂõûÂêàÊ≥ïÁöÑ JSONÔºå‰∏çË¶ÅÂÖ∂‰ªñÊñáÂ≠ó
- Á°Æ‰øùÊâÄÊúâÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑÂèåÂºïÂè∑Áî®ÂçïÂºïÂè∑ÊõøÊç¢"""


def _call_minimax(prompt: str, label: str) -> str:
    """Call MiniMax M2.1 API and return response text. Returns None on failure."""
    import time
    import re
    from openai import OpenAI as OpenAIClient

    api_key = os.environ.get("MINIMAX_API_KEY")
    if not api_key:
        return None

    client = OpenAIClient(api_key=api_key, base_url="https://api.minimaxi.chat/v1")
    start = time.time()
    try:
        resp = client.chat.completions.create(
            model="MiniMax-M2.1",
            max_tokens=8192,
            messages=[{"role": "user", "content": prompt}],
        )
        elapsed = time.time() - start
        print(f"  - MiniMax ({label}) ËÄóÊó∂: {elapsed:.1f}s")
        text = resp.choices[0].message.content
        # Strip <think> tags if present
        text = re.sub(r'<think>[\s\S]*?</think>', '', text).strip()
        return text
    except Exception as e:
        print(f"  - MiniMax ({label}) error: {e}")
        return None


def _parse_json_response(response_text: str):
    """Extract and parse JSON from model response text. Returns parsed dict or None.

    Uses multi-pass JSON repair matching the main summarize function.
    """
    import re
    start_idx = response_text.find('{')
    end_idx = response_text.rfind('}') + 1
    if start_idx == -1 or end_idx <= start_idx:
        return None
    json_str = response_text[start_idx:end_idx]

    # Pass 1: direct parse
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as first_error:
        print(f"  - JSON parse error (attempting fix): {first_error}")

    # Pass 2: control chars + unescaped quotes fix
    json_str = re.sub(r'[\x00-\x1f\x7f]', ' ', json_str)

    def fix_quotes_in_value(match):
        key = match.group(1)
        value = match.group(2)
        fixed_value = value.replace('"', "'")
        return f'"{key}": "{fixed_value}"'

    json_str = re.sub(
        r'"(title|summary|comment|source|url|name|icon)"\s*:\s*"((?:[^"\\]|\\.)*)(?<!\\)"',
        fix_quotes_in_value,
        json_str
    )

    try:
        return json.loads(json_str)
    except json.JSONDecodeError as second_error:
        print(f"  - JSON fix attempt 1 failed: {second_error}")

    # Pass 3: trailing commas + extract categories array
    json_str = re.sub(r',\s*}', '}', json_str)
    json_str = re.sub(r',\s*]', ']', json_str)

    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        pass

    try:
        cat_match = re.search(r'"categories"\s*:\s*(\[[\s\S]*\])', json_str)
        if cat_match:
            categories_str = cat_match.group(1)
            categories_str = re.sub(r',\s*}', '}', categories_str)
            categories_str = re.sub(r',\s*]', ']', categories_str)
            result = json.loads(categories_str)
            print(f"  - Recovered {len(result)} categories from partial JSON")
            return {"categories": result}
    except Exception:
        pass

    # Pass 4: line-by-line quote reconstruction
    try:
        lines = json_str.split('\n')
        fixed_lines = []
        for line in lines:
            m = re.match(r'^(\s*"(?:title|summary|comment|source|url|name|icon)":\s*")(.*)(",?\s*)$', line)
            if m:
                value = m.group(2).replace('"', "'")
                line = m.group(1) + value + m.group(3)
            fixed_lines.append(line)
        json_str = '\n'.join(fixed_lines)
        return json.loads(json_str)
    except Exception as final_error:
        print(f"  - All JSON fix attempts failed: {final_error}")
        return None


def _call_haiku(client, prompt: str, label: str) -> str:
    """Call Claude Haiku and return response text. Returns None on failure."""
    import time
    try:
        resp = client.messages.create(
            model="claude-haiku-4-5-20251001",
            max_tokens=8192,
            messages=[{"role": "user", "content": prompt}],
        )
        elapsed = time.time()
        print(f"  - Haiku ({label}) stop_reason: {resp.stop_reason}")
        if resp.stop_reason == "max_tokens":
            print(f"  - WARNING: Response was truncated (hit max_tokens)")
        return resp.content[0].text
    except Exception as e:
        print(f"  - Haiku ({label}) error: {e}")
        return None


def _format_articles_text(articles: list[dict]) -> str:
    """Format a list of article dicts into text for Claude prompts."""
    text = ""
    for i, article in enumerate(articles, 1):
        text += f"""
---
Article {i}:
Title: {article.get('title', '')}
Source: {article.get('source', '')}
Published: {article.get('published', '')}
Description: {article.get('description', '')}
URL: {article.get('url', '')}
"""
    return text


def _focused_split_call(client, articles: list[dict], max_items: int, paywalled_sources: str, settings: dict) -> list[dict]:
    """Focused mode: two sequential Haiku calls for hardware and AI/industry, then merge."""
    import time

    print(f"  - Focused mode: 2 Haiku calls (hardware + AI/industry)")

    # Split articles: hardware sources vs others
    hw_urls = set()
    for feed in settings.get("rss_feeds", []):
        if feed.get("group") == "Êô∫ËÉΩÁ°¨‰ª∂" and feed.get("enabled", True):
            hw_urls.add(feed.get("url", ""))

    hw_articles = [a for a in articles if a.get("feed_url", "") in hw_urls]
    other_articles = [a for a in articles if a.get("feed_url", "") not in hw_urls]
    print(f"  - Article split: {len(hw_articles)} hardware, {len(other_articles)} other")

    hw_articles_text = _format_articles_text(hw_articles) if hw_articles else _format_articles_text(articles)
    other_articles_text = _format_articles_text(other_articles) if other_articles else _format_articles_text(articles)

    hw_budget = 7  # hardware gets 5-7 items
    ai_budget = max(max_items - hw_budget, 3)  # rest goes to AI+industry, at least 3
    prompt_hw = get_prompt_for_mode("focused_hardware", hw_articles_text, max_items, "", "", "", None, paywalled_sources)
    prompt_ai = get_prompt_for_mode("focused_ai_industry", other_articles_text, ai_budget, "", "", "", None, paywalled_sources)
    print(f"  - Budget: hardware 5-7, AI+industry {ai_budget}, total cap {max_items}")

    start = time.time()

    def _call_and_parse(prompt, label, max_retries=2):
        """Call Haiku and parse JSON, with retries for both API and parse failures."""
        for attempt in range(max_retries + 1):
            if attempt > 0:
                print(f"  - Retrying {label} (attempt {attempt + 1})...")
                time.sleep(3)
            resp = _call_haiku(client, prompt, f"{label}" if attempt == 0 else f"{label}-retry{attempt}")
            if not resp:
                print(f"  - {label}: API call returned None")
                continue
            parsed = _parse_json_response(resp)
            if parsed:
                return parsed
            print(f"  - {label}: JSON parse failed. Preview: {resp[:200]}")
        return None

    hw_parsed = _call_and_parse(prompt_hw, "Êô∫ËÉΩÁ°¨‰ª∂")
    ai_parsed = _call_and_parse(prompt_ai, "AI+Ë°å‰∏ö")

    elapsed = time.time() - start
    print(f"  - Focused split total ËÄóÊó∂: {elapsed:.1f}s")

    categories = []

    # Parse hardware result
    if hw_parsed:
        hw_news = hw_parsed.get("news", [])
        if hw_news:
            categories.append({"name": "Êô∫ËÉΩÁ°¨‰ª∂", "icon": "ü•Ω", "news": hw_news})
            print(f"  - ü•Ω Êô∫ËÉΩÁ°¨‰ª∂: {len(hw_news)} Êù°")
        else:
            print(f"  - ü•Ω Êô∫ËÉΩÁ°¨‰ª∂: parsed OK but no 'news' key. Keys: {list(hw_parsed.keys())}")
    else:
        print(f"  - ü•Ω Êô∫ËÉΩÁ°¨‰ª∂: all attempts failed")

    # Collect URLs from hardware for dedup
    seen_urls = set()
    for cat in categories:
        for news in cat.get("news", []):
            url = news.get("url", "")
            if url:
                seen_urls.add(url)

    if ai_parsed:
        ai_cats = ai_parsed.get("categories", [])
        print(f"  - AI+Ë°å‰∏ö: parsed OK, {len(ai_cats)} categories. Keys: {list(ai_parsed.keys())}")
        for cat in ai_cats:
            cat_name = cat.get("name", "?")
            cat_news = cat.get("news", [])
            deduped_news = [n for n in cat_news if n.get("url", "") not in seen_urls]
            removed = len(cat_news) - len(deduped_news)
            if deduped_news:
                categories.append({"name": cat_name, "icon": cat.get("icon", ""), "news": deduped_news})
                msg = f"  - {cat.get('icon', '')} {cat_name}: {len(deduped_news)} Êù°"
                if removed > 0:
                    msg += f" (ÂéªÈáçÁßªÈô§ {removed} Êù°)"
                print(msg)
            else:
                print(f"  - {cat.get('icon', '')} {cat_name}: 0 Êù° (all {len(cat_news)} deduped)")
    else:
        print(f"  - AI+Ë°å‰∏ö: all attempts failed")

    total = sum(len(c.get("news", [])) for c in categories)
    print(f"  - Focused total: {total} items in {len(categories)} categories")

    if not categories:
        print(f"  - WARNING: Both calls failed, returning empty")

    return categories


def summarize_news_with_claude(anthropic_key: str, articles: list[dict], max_items: int = 10, settings: dict = None) -> list[dict]:
    """Use Claude to summarize, categorize, and select top news."""

    if not articles:
        return []

    if settings is None:
        settings = load_settings()

    topic_mode = settings.get("topic_mode", "broad")  # "broad" or "focused"
    custom_prompt = settings.get("custom_prompt", "")  # User-defined custom prompt
    client = anthropic.Anthropic(api_key=anthropic_key)

    # ËÅöÁÑ¶Ê®°Âºè‰ΩøÁî®‰∏ìÈó®ÁöÑ 3 ‰∏™ÂàÜÁ±ª
    if topic_mode == "focused" and not custom_prompt:
        categories = [
            {"name": "Êô∫ËÉΩÁ°¨‰ª∂", "icon": "ü•Ω"},
            {"name": "AIÊäÄÊúØ‰∏é‰∫ßÂìÅ", "icon": "ü§ñ"},
            {"name": "Â∑®Â§¥Âä®Âêë‰∏éË°å‰∏öËßÇÂØü", "icon": "üè¢"},
        ]
    else:
        categories = get_categories(settings)

    if custom_prompt:
        print(f"  - Using custom prompt ({len(custom_prompt)} chars)")
    else:
        print(f"  - Topic mode: {topic_mode}")

    # Prepare articles for Claude
    articles_text = ""
    for i, article in enumerate(articles[:120], 1):  # Limit to 120 articles for diversity
        articles_text += f"""
---
Article {i}:
Title: {article.get('title', '')}
Source: {article.get('source', '')}
Published: {article.get('published', '')}
Description: {article.get('description', '')}
URL: {article.get('url', '')}
"""

    category_names = "„ÄÅ".join(c["name"] for c in categories)
    category_json_example = json.dumps(
        [{"name": c["name"], "icon": c["icon"], "news": [{"title": "...", "summary": "...", "comment": "‰∏Ä‰∏™ÂêØÂèëÊÄùËÄÉÁöÑÈóÆÈ¢òÔºü", "source": "...", "url": "..."}]} for c in categories[:2]],
        ensure_ascii=False, indent=4
    )

    icon_mapping = " ".join(f'{c["name"]}:{c["icon"]}' for c in categories)

    # Ëé∑Âèñ‰ªòË¥πÂ¢ôÊ∫êÂêçÁß∞
    rss_feeds = settings.get("rss_feeds", [])
    paywalled_sources = ", ".join(
        feed.get("name", "") for feed in rss_feeds
        if feed.get("paywalled", False) and feed.get("enabled", True)
    )
    if paywalled_sources:
        print(f"  - Paywalled sources: {paywalled_sources}")

    prompt = get_prompt_for_mode(topic_mode, articles_text, max_items, category_names, category_json_example, icon_mapping, custom_prompt, paywalled_sources)

    import time
    claude_start = time.time()

    # Focused mode: split into 2 parallel calls (hardware + AI/industry)
    if prompt is None and topic_mode == "focused":
        return _focused_split_call(client, articles[:120], max_items, paywalled_sources, settings)

    model = "claude-haiku-4-5-20251001"
    print(f"  - Using model: {model}")

    try:
        response = client.messages.create(
            model=model,
            max_tokens=8192,
            messages=[{"role": "user", "content": prompt}]
        )
        claude_elapsed = time.time() - claude_start
        print(f"  - Claude API ({topic_mode}) ËÄóÊó∂: {claude_elapsed:.1f}s")

        if response.stop_reason == "max_tokens":
            print(f"  - WARNING: Response was truncated (hit max_tokens limit)")

        response_text = response.content[0].text

        # Extract JSON from response
        start_idx = response_text.find('{')
        end_idx = response_text.rfind('}') + 1
        if start_idx != -1 and end_idx > start_idx:
            json_str = response_text[start_idx:end_idx]

            # Try parsing directly first
            try:
                result = json.loads(json_str)
                return result.get("categories", [])
            except json.JSONDecodeError as first_error:
                print(f"  - JSON parse error (attempting fix): {first_error}")
                # Debug: show the problematic area
                error_pos = first_error.pos if hasattr(first_error, 'pos') else 0
                start_show = max(0, error_pos - 100)
                end_show = min(len(json_str), error_pos + 100)
                print(f"  - Error context (pos {error_pos}): ...{json_str[start_show:end_show]}...")

            # Fix common JSON issues
            import re

            # Remove control characters
            json_str = re.sub(r'[\x00-\x1f\x7f]', ' ', json_str)

            # Fix unescaped quotes inside string values
            # Match: "key": "value with "quotes" inside"
            def fix_quotes_in_value(match):
                key = match.group(1)
                value = match.group(2)
                # Replace inner quotes with single quotes
                fixed_value = value.replace('"', "'")
                return f'"{key}": "{fixed_value}"'

            # Pattern for string fields
            json_str = re.sub(
                r'"(title|summary|comment|source|url|name|icon)"\s*:\s*"((?:[^"\\]|\\.)*)(?<!\\)"',
                fix_quotes_in_value,
                json_str
            )

            # Try again
            try:
                result = json.loads(json_str)
                return result.get("categories", [])
            except json.JSONDecodeError as second_error:
                print(f"  - JSON fix attempt 1 failed: {second_error}")

            # More aggressive fix: use ast.literal_eval style parsing
            # Replace problematic patterns
            json_str = re.sub(r',\s*}', '}', json_str)  # trailing comma before }
            json_str = re.sub(r',\s*]', ']', json_str)  # trailing comma before ]

            # Try with relaxed JSON parser
            try:
                # Try to extract just the categories array if full parse fails
                cat_match = re.search(r'"categories"\s*:\s*(\[[\s\S]*\])', json_str)
                if cat_match:
                    categories_str = cat_match.group(1)
                    # Clean up the categories string
                    categories_str = re.sub(r',\s*}', '}', categories_str)
                    categories_str = re.sub(r',\s*]', ']', categories_str)
                    result = json.loads(categories_str)
                    print(f"  - Recovered {len(result)} categories from partial JSON")
                    return result
            except Exception as third_error:
                print(f"  - JSON fix attempt 2 failed: {third_error}")

            # Last resort: try line by line reconstruction
            try:
                lines = json_str.split('\n')
                fixed_lines = []
                for line in lines:
                    m = re.match(r'^(\s*"(?:title|summary|comment|source|url|name|icon)":\s*")(.*)(",?\s*)$', line)
                    if m:
                        value = m.group(2).replace('"', "'")
                        line = m.group(1) + value + m.group(3)
                    fixed_lines.append(line)
                json_str = '\n'.join(fixed_lines)
                result = json.loads(json_str)
                return result.get("categories", [])
            except Exception as final_error:
                print(f"  - All JSON fix attempts failed: {final_error}")
    except Exception as e:
        print(f"  Error: Failed to summarize news: {e}")

    return []

def fetch_news(anthropic_key: str, topic: str = "AI/ÁßëÊäÄ", max_items: int = 10, settings: dict = None, manual: bool = False, hardware_unlimited: bool = None, channel: dict = None) -> dict:
    """Fetch and process news.

    Args:
        anthropic_key: API key for Claude
        topic: News topic
        max_items: Maximum news items to return
        settings: Configuration dict
        manual: If True, use current time as window end (manual trigger)
        hardware_unlimited: Override for hardware source limiting. If None, auto-detect from topic_mode.
        channel: Optional channel dict for time window calculation.

    Returns dict with categories and _raw_articles (for multi-channel reuse).
    """

    if settings is None:
        settings = load_settings()

    tz_name = settings.get("timezone", "Asia/Shanghai")
    tz = ZoneInfo(tz_name)
    today = datetime.now(tz).strftime("%Y-%m-%d")
    start_time, end_time = get_time_window(settings, manual=manual, channel=channel)
    cutoff = get_cutoff_time(settings, manual=manual, channel=channel)

    print(f"  - Time window: {start_time} ~ {end_time}")

    # ËÅöÁÑ¶Ê®°Âºè‰∏ãÔºåÊô∫ËÉΩÁ°¨‰ª∂Ê∫ê‰∏çÂèóÊï∞ÈáèÈôêÂà∂
    if hardware_unlimited is None:
        topic_mode = settings.get("topic_mode", "broad")
        hardware_unlimited = (topic_mode == "focused")

    print("  - Fetching news from RSS feeds...")
    raw_articles = fetch_raw_news(cutoff=cutoff, settings=settings, hardware_unlimited=hardware_unlimited)
    print(f"  - Got {len(raw_articles)} raw articles")

    # Apply blacklist/whitelist filters
    raw_articles = apply_filters(raw_articles, settings)
    print(f"  - After filtering: {len(raw_articles)} articles")

    if not raw_articles:
        return {
            "date": today,
            "time_window": f"{start_time} ~ {end_time}",
            "categories": [],
            "_raw_articles": [],
            "error": "No articles fetched from RSS feeds"
        }

    print("  - Summarizing with Claude...")
    categories = summarize_news_with_claude(anthropic_key, raw_articles, max_items, settings)
    total = sum(len(c.get("news", [])) for c in categories)
    print(f"  - Selected {total} top news in {len(categories)} categories")

    return {
        "date": today,
        "time_window": f"{start_time} ~ {end_time}",
        "categories": categories,
        "_raw_articles": raw_articles,
    }

def save_draft(news_data: dict, settings: dict = None, channel_id: str = None) -> str:
    """Save news data as a draft JSON file.

    Args:
        news_data: The news data dict (categories, date, etc.)
        settings: Configuration dict
        channel_id: If set, saves as a channel-specific draft (YYYY-MM-DD_ch_<id>.json)

    Returns the draft file path.
    """
    if settings is None:
        settings = load_settings()

    date = news_data.get("date", datetime.now().strftime("%Y-%m-%d"))
    drafts_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config", "drafts")
    os.makedirs(drafts_dir, exist_ok=True)

    if channel_id:
        filename = f"{date}_ch_{channel_id}.json"
    else:
        filename = f"{date}.json"
    draft_path = os.path.join(drafts_dir, filename)

    # Never overwrite a draft that's already been sent or rejected
    if os.path.exists(draft_path):
        try:
            with open(draft_path, "r", encoding="utf-8") as f:
                existing = json.load(f)
            existing_status = existing.get("status", "")
            if existing_status in ("sent", "rejected"):
                print(f"  - Skipping {filename}: already {existing_status}")
                return draft_path
        except (json.JSONDecodeError, IOError):
            pass  # Corrupted file, safe to overwrite

    # Filter out internal fields like _raw_articles
    clean_data = {k: v for k, v in news_data.items() if not k.startswith("_")}

    draft_data = {
        **clean_data,
        "status": news_data.get("status", "pending_review"),
        "source": news_data.get("source", "scheduled"),
        "created_at": datetime.now(ZoneInfo(settings.get("timezone", "Asia/Shanghai"))).isoformat(),
    }

    # Add channel metadata for channel drafts
    if channel_id:
        draft_data["channel_id"] = channel_id
        # Find channel config to store name and topic_mode
        all_channels = settings.get("channels", settings.get("webhook_channels", []))
        for ch in all_channels:
            if ch.get("id") == channel_id:
                draft_data["channel_name"] = ch.get("name", "")
                draft_data["topic_mode"] = ch.get("topic_mode", "broad")
                break

    with open(draft_path, "w", encoding="utf-8") as f:
        json.dump(draft_data, f, ensure_ascii=False, indent=2)

    print(f"  - Draft saved to {draft_path}")

    # Ê∏ÖÁêÜ 30 Â§©ÂâçÁöÑÊóßËçâÁ®ø
    cleanup_old_drafts(drafts_dir, days=30)

    return draft_path


def cleanup_old_drafts(drafts_dir: str, days: int = 30):
    """Delete draft files older than specified days.

    Handles both YYYY-MM-DD.json and YYYY-MM-DD_ch_<id>.json formats.
    """
    cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    deleted = []

    try:
        for filename in os.listdir(drafts_dir):
            if not filename.endswith('.json'):
                continue
            # Extract date from filename: YYYY-MM-DD.json or YYYY-MM-DD_ch_xxx.json
            base = filename.replace('.json', '')
            # Date is always the first 10 chars (YYYY-MM-DD)
            file_date = base[:10]
            if len(file_date) == 10 and file_date < cutoff_date:
                filepath = os.path.join(drafts_dir, filename)
                os.remove(filepath)
                deleted.append(filename)
    except Exception as e:
        print(f"  Warning: Failed to cleanup old drafts: {e}")

    if deleted:
        print(f"  - Cleaned up {len(deleted)} old drafts: {deleted}")

def load_draft(date: str = None, channel_id: str = None):
    """Load a draft by date and optional channel_id.

    Args:
        date: Date string (YYYY-MM-DD). Defaults to today.
        channel_id: If set, loads the channel-specific draft.

    Returns the draft data dict, or None if not found.
    """
    if date is None:
        settings = load_settings()
        tz = ZoneInfo(settings.get("timezone", "Asia/Shanghai"))
        date = datetime.now(tz).strftime("%Y-%m-%d")

    drafts_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config", "drafts")

    if channel_id:
        filename = f"{date}_ch_{channel_id}.json"
    else:
        filename = f"{date}.json"
    draft_path = os.path.join(drafts_dir, filename)

    try:
        with open(draft_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return None

def format_email_html(news_data: dict, settings: dict = None) -> str:
    """Format news data into a beautiful HTML email.

    Categories are rendered in the order from the draft (ËÅöÁÑ¶Ê®°ÂºèÁöÑÈ°∫Â∫èÁî± Claude ËøîÂõû).
    """
    if settings is None:
        settings = load_settings()

    date = news_data.get("date", "")
    time_window = news_data.get("time_window", "")
    raw_categories = news_data.get("categories", [])

    # Build category sections - Áõ¥Êé•ÊåâËçâÁ®ø‰∏≠ÁöÑÈ°∫Â∫èÊòæÁ§∫
    sections_html = ""
    has_news = False
    for cat in raw_categories:
        cat_name = cat.get("name", "")
        news_items = cat.get("news", [])
        if not news_items:
            continue
        has_news = True
        icon = CATEGORY_ICONS.get(cat_name, cat.get("icon", "üì∞"))

        cards_html = ""
        for item in news_items:
            title = item.get("title", "")
            summary = item.get("summary", "")
            comment = item.get("comment", "")
            source = item.get("source", "")
            url = item.get("url", "#")

            comment_html = ""
            if comment:
                comment_html = f'<p style="color:#7c3aed;font-size:13px;line-height:1.5;margin:8px 0 10px 0;padding:8px 12px;background:#f5f3ff;border-radius:6px;">ü§î {comment}</p>'

            cards_html += f'''<table width="100%" cellpadding="0" cellspacing="0" border="0" style="margin-bottom:12px;">
<tr><td style="background:#ffffff;border-radius:8px;border:1px solid #e8e8e8;padding:16px 20px;">
  <a href="{url}" style="color:#1a1a2e;text-decoration:none;font-size:15px;font-weight:600;line-height:1.4;display:block;" target="_blank">{title}</a>
  <p style="color:#555;font-size:14px;line-height:1.6;margin:8px 0 10px 0;">{summary}</p>
  {comment_html}
  <span style="display:inline-block;background:#eef2ff;color:#4f46e5;font-size:12px;padding:2px 10px;border-radius:12px;">{source}</span>
</td></tr>
</table>'''

        sections_html += f'''<tr><td style="padding:24px 30px 8px 30px;">
  <h2 style="margin:0 0 16px 0;font-size:18px;color:#1a1a2e;font-weight:700;">{icon} {cat_name}</h2>
  {cards_html}
</td></tr>'''

    if not has_news:
        sections_html = '<tr><td style="padding:20px 30px;color:#666;font-size:16px;">‰ªäÊó•ÊöÇÊó†ÈáçË¶ÅÊñ∞Èóª„ÄÇ</td></tr>'

    html = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"></head>
<body style="margin:0;padding:0;background-color:#f0f2f5;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI','PingFang SC','Hiragino Sans GB','Microsoft YaHei',sans-serif;">
<table width="100%" cellpadding="0" cellspacing="0" border="0" style="background-color:#f0f2f5;">
<tr><td align="center" style="padding:24px 16px;">
<table width="100%" cellpadding="0" cellspacing="0" border="0" style="max-width:640px;background-color:#ffffff;border-radius:12px;overflow:hidden;box-shadow:0 2px 12px rgba(0,0,0,0.08);">

<!-- Header -->
<tr><td style="background:linear-gradient(135deg,#1a1a2e 0%,#16213e 50%,#0f3460 100%);padding:32px 30px;text-align:center;">
  <h1 style="margin:0;color:#ffffff;font-size:22px;font-weight:700;letter-spacing:1px;">AI / ÁßëÊäÄÊñ∞ÈóªÊó•Êä•</h1>
  <p style="margin:10px 0 0 0;color:rgba(255,255,255,0.75);font-size:14px;">{date} &nbsp;|&nbsp; {time_window}</p>
</td></tr>

<!-- News Sections -->
{sections_html}

<!-- Footer -->
<tr><td style="padding:20px 30px;border-top:1px solid #eee;text-align:center;">
  <p style="margin:0;color:#999;font-size:12px;">Áî± AI News Assistant Ëá™Âä®ÁîüÊàê &nbsp;&middot;&nbsp; Powered by Claude</p>
</td></tr>

</table>
</td></tr>
</table>
</body>
</html>'''

    return html

if __name__ == "__main__":
    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")

    if not anthropic_key:
        print("Error: ANTHROPIC_API_KEY environment variable not set")
        exit(1)

    news_data = fetch_news(anthropic_key)
    print(json.dumps(news_data, ensure_ascii=False, indent=2))
