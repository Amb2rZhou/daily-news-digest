#!/usr/bin/env python3
"""
Fetch AI/Tech news using RSS feeds and summarize with Claude.
"""

try:
    import anthropic
except ImportError:
    anthropic = None
import feedparser
import json
import os
import requests
from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo
from concurrent.futures import ThreadPoolExecutor, as_completed

# Fallback RSS feeds (used when settings.json has no rss_feeds)
DEFAULT_RSS_FEEDS = [
    "https://techcrunch.com/feed/",
    "https://www.theverge.com/rss/index.xml",
    "https://feeds.arstechnica.com/arstechnica/technology-lab",
    "https://www.wired.com/feed/rss",
    "https://venturebeat.com/feed/",
    "https://www.technologyreview.com/feed/",
    "https://openai.com/blog/rss.xml",
    "https://blog.google/technology/ai/rss/",
    "https://ai.meta.com/blog/rss/",
    "https://www.anthropic.com/rss.xml",
    "https://hnrss.org/frontpage",
    "https://www.reddit.com/r/MachineLearning/.rss",
    "https://36kr.com/feed",
    "https://www.jiqizhixin.com/rss",
    "https://www.huxiu.com/rss/0.xml",
    "https://www.tmtpost.com/feed",
    "https://www.pingwest.com/feed",
    "https://www.geekpark.net/rss",
    "https://github.blog/feed/",
    "https://a16z.com/feed/",
]

def get_rss_feeds(settings: dict = None) -> list[str]:
    """Get RSS feed URLs from settings (enabled only), with fallback to defaults."""
    if settings is None:
        settings = load_settings()
    rss_feeds = settings.get("rss_feeds", [])
    if rss_feeds:
        return [f["url"] for f in rss_feeds if f.get("enabled", True)]
    return DEFAULT_RSS_FEEDS

# Default config path (relative to project root)
CONFIG_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config", "settings.json")

def load_settings() -> dict:
    """Load settings from config/settings.json.

    Backward-compatible: auto-migrates old formats to the new unified
    ``channels`` array.  Supports three legacy shapes:

    1. ``webhook_channels`` present (no ``channels``)  â†’ convert
    2. ``webhook_enabled`` present (no ``webhook_channels``, no ``channels``) â†’ convert
    3. Only top-level ``send_hour``/``send_minute``/``topic_mode``/``max_news_items`` â†’ convert
    """
    defaults = {
        "timezone": "Asia/Shanghai",
        "categories_order": ["äº§å“å‘å¸ƒ", "å·¨å¤´åŠ¨å‘", "æŠ€æœ¯è¿›å±•", "è¡Œä¸šè§‚å¯Ÿ", "æŠ•èèµ„"],
        "filters": {
            "blacklist_keywords": [],
            "blacklist_sources": [],
            "whitelist_keywords": [],
            "whitelist_sources": []
        }
    }
    config_path = os.environ.get("SETTINGS_PATH", CONFIG_PATH)
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            settings = json.load(f)
        # Merge with defaults for any missing keys
        for k, v in defaults.items():
            settings.setdefault(k, v)

        # --- Backward-compatible migration to unified channels ---
        if "channels" not in settings:
            send_hour = settings.get("send_hour", 10)
            send_minute = settings.get("send_minute", 0)
            topic_mode = settings.get("topic_mode", "broad")
            max_items = settings.get("max_news_items", 10)

            channels = []

            # Email channel (always present)
            channels.append({
                "id": "email",
                "type": "email",
                "name": "é‚®ä»¶",
                "enabled": True,
                "send_hour": send_hour,
                "send_minute": send_minute,
                "topic_mode": topic_mode,
                "max_news_items": max_items,
            })

            # Migrate webhook_channels or webhook_enabled
            if "webhook_channels" in settings:
                for ch in settings["webhook_channels"]:
                    channels.append({
                        "id": ch.get("id", "default"),
                        "type": "webhook",
                        "name": ch.get("name", "é»˜è®¤ç¾¤"),
                        "enabled": ch.get("enabled", False),
                        "send_hour": send_hour,
                        "send_minute": send_minute,
                        "topic_mode": ch.get("topic_mode", topic_mode),
                        "max_news_items": max_items,
                        "webhook_url_base": ch.get("webhook_url_base", ""),
                    })
            elif settings.get("webhook_enabled", False):
                channels.append({
                    "id": "default",
                    "type": "webhook",
                    "name": "é»˜è®¤ç¾¤",
                    "enabled": True,
                    "send_hour": send_hour,
                    "send_minute": send_minute,
                    "topic_mode": topic_mode,
                    "max_news_items": max_items,
                    "webhook_url_base": "",
                })

            settings["channels"] = channels

        return settings
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"  Warning: Could not load settings from {config_path}: {e}")
        return defaults

CATEGORY_ICONS = {
    # èšç„¦æ¨¡å¼çš„ 3 ä¸ªåˆ†ç±»
    "æ™ºèƒ½ç¡¬ä»¶": "ğŸ¥½",
    "AIæŠ€æœ¯ä¸äº§å“": "ğŸ¤–",
    "å·¨å¤´åŠ¨å‘ä¸è¡Œä¸šè§‚å¯Ÿ": "ğŸ¢",
    # æ³› AI æ¨¡å¼çš„ 5 ä¸ªåˆ†ç±»ï¼ˆä¿ç•™å…¼å®¹ï¼‰
    "äº§å“å‘å¸ƒ": "ğŸš€",
    "å·¨å¤´åŠ¨å‘": "ğŸ¢",
    "æŠ€æœ¯è¿›å±•": "ğŸ”¬",
    "è¡Œä¸šè§‚å¯Ÿ": "ğŸ“Š",
    "æŠ•èèµ„": "ğŸ’°",
}

def get_categories(settings: dict = None) -> list[dict]:
    """Get ordered category list from settings."""
    if settings is None:
        settings = load_settings()
    order = settings.get("categories_order", list(CATEGORY_ICONS.keys()))
    return [{"name": name, "icon": CATEGORY_ICONS.get(name, "ğŸ“°")} for name in order if name in CATEGORY_ICONS]

def get_time_window(settings: dict = None, manual: bool = False, channel: dict = None) -> tuple[str, str]:
    """Calculate the news time window.

    Args:
        settings: Configuration dict
        manual: If True, window ends at current time (for manual trigger)
                If False, window ends at scheduled send time (for auto trigger)
        channel: Optional channel dict â€“ uses its send_hour/send_minute if given.

    Returns:
        Tuple of (start_time, end_time) as formatted strings
    """
    if settings is None:
        settings = load_settings()

    if channel:
        send_hour = channel.get("send_hour", 10)
        send_minute = channel.get("send_minute", 0)
    else:
        # Fallback: use the first channel's time, or defaults
        channels = settings.get("channels", [])
        first = channels[0] if channels else {}
        send_hour = first.get("send_hour", settings.get("send_hour", 10))
        send_minute = first.get("send_minute", settings.get("send_minute", 0))

    tz_name = settings.get("timezone", "Asia/Shanghai")
    tz = ZoneInfo(tz_name)

    now = datetime.now(tz)

    if manual:
        # Manual trigger: window ends at current time
        end_time = now
    else:
        # Auto trigger: window ends at today's scheduled send time
        # Fetch always happens shortly before send_time, so use today's send_time
        end_time = now.replace(hour=send_hour, minute=send_minute, second=0, microsecond=0)

    start_time = end_time - timedelta(days=1)

    return (
        start_time.strftime("%Y-%m-%d %H:%M"),
        (end_time - timedelta(seconds=1)).strftime("%Y-%m-%d %H:%M")
    )

def get_cutoff_time(settings: dict = None, manual: bool = False, channel: dict = None) -> datetime:
    """Get the cutoff time for filtering articles.

    Args:
        settings: Configuration dict
        manual: If True, cutoff is 24h before now (for manual trigger)
                If False, cutoff is 24h before scheduled send time (for auto trigger)
        channel: Optional channel dict â€“ uses its send_hour/send_minute if given.
    """
    if settings is None:
        settings = load_settings()
    tz_name = settings.get("timezone", "Asia/Shanghai")
    tz = ZoneInfo(tz_name)

    if channel:
        send_hour = channel.get("send_hour", 10)
        send_minute = channel.get("send_minute", 0)
    else:
        channels = settings.get("channels", [])
        first = channels[0] if channels else {}
        send_hour = first.get("send_hour", settings.get("send_hour", 10))
        send_minute = first.get("send_minute", settings.get("send_minute", 0))

    now = datetime.now(tz)

    if manual:
        # Manual trigger: 24h before now
        # Convert to UTC before stripping tzinfo (RSS published_parsed is naive UTC)
        cutoff = now - timedelta(days=1)
        return cutoff.astimezone(ZoneInfo("UTC")).replace(tzinfo=None)
    else:
        # Auto trigger: 24h before today's scheduled send time
        today_send = now.replace(hour=send_hour, minute=send_minute, second=0, microsecond=0)
        cutoff = today_send - timedelta(days=1)
        return cutoff.astimezone(ZoneInfo("UTC")).replace(tzinfo=None)

def parse_feed(feed_url: str, cutoff: datetime = None) -> list[dict]:
    """Parse a single RSS feed and return recent articles."""
    articles = []
    if cutoff is None:
        cutoff = datetime.now() - timedelta(hours=24)

    try:
        # Use requests to fetch content first (handles SSL better than feedparser's urllib)
        try:
            resp = requests.get(feed_url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            resp.raise_for_status()
            feed = feedparser.parse(resp.content)
        except requests.RequestException:
            # Don't fallback to feedparser.parse(url) â€” it has no timeout and can hang
            return []
        source_name = feed.feed.get("title", feed_url)

        for entry in feed.entries[:20]:  # Limit entries per feed
            # Parse published time
            published = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6])
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                published = datetime(*entry.updated_parsed[:6])

            # Skip if too old or no date
            if published and published < cutoff:
                continue

            articles.append({
                "title": entry.get("title", ""),
                "description": entry.get("summary", entry.get("description", ""))[:500],
                "source": source_name,
                "feed_url": feed_url,
                "url": entry.get("link", ""),
                "published": published.isoformat() if published else ""
            })
    except Exception as e:
        print(f"  Warning: Failed to parse {feed_url}: {e}")

    return articles

def fetch_raw_news(cutoff: datetime = None, settings: dict = None, max_per_source: int = 3, hardware_unlimited: bool = False) -> list[dict]:
    """Fetch raw news from multiple RSS feeds in parallel.

    Args:
        cutoff: Only include articles published after this time
        settings: Settings dict
        max_per_source: Maximum articles to keep per source (ensures diversity)
        hardware_unlimited: If True, smart hardware sources are not limited (only for focused mode)
    """
    if settings is None:
        settings = load_settings()

    all_articles = []
    feed_urls = get_rss_feeds(settings)
    print(f"  - Using {len(feed_urls)} RSS feeds")

    # Build feed_url â†’ group mapping and per-group article limits
    rss_feeds = settings.get("rss_feeds", [])
    source_limits = settings.get("source_limits", {})
    default_limit = source_limits.get("default", max_per_source)
    feed_url_to_group = {}
    for feed in rss_feeds:
        if feed.get("enabled", True):
            feed_url_to_group[feed.get("url", "")] = feed.get("group", "")

    # è·å–æ™ºèƒ½ç¡¬ä»¶æºçš„ URL åˆ—è¡¨ï¼ˆä»…èšç„¦æ¨¡å¼ä¸‹ä¸å—é™åˆ¶ï¼‰
    hardware_urls = set()
    if hardware_unlimited:
        for feed in rss_feeds:
            if feed.get("group") == "æ™ºèƒ½ç¡¬ä»¶" and feed.get("enabled", True):
                hardware_urls.add(feed.get("url", ""))
        print(f"  - Smart hardware sources (no limit): {len(hardware_urls)} feeds")

    # Collect articles grouped by source
    articles_by_source = {}

    failed_feeds = []
    timeout_feeds = []
    empty_feeds = []

    import time
    rss_start = time.time()

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(parse_feed, url, cutoff): url for url in feed_urls}

        for future in as_completed(futures):
            url = futures[future]
            try:
                articles = future.result()
                if not articles:
                    empty_feeds.append(url)
                for article in articles:
                    source = article.get("source", "unknown")
                    if source not in articles_by_source:
                        articles_by_source[source] = []
                    articles_by_source[source].append(article)
            except Exception as e:
                err_str = str(e).lower()
                if 'timeout' in err_str or 'timed out' in err_str:
                    timeout_feeds.append(url)
                else:
                    failed_feeds.append((url, str(e)))

    rss_elapsed = time.time() - rss_start
    print(f"  - RSS æŠ“å–è€—æ—¶: {rss_elapsed:.1f}s")
    print(f"  - æˆåŠŸ: {len(feed_urls) - len(failed_feeds) - len(timeout_feeds) - len(empty_feeds)}, ç©º: {len(empty_feeds)}, è¶…æ—¶: {len(timeout_feeds)}, å¤±è´¥: {len(failed_feeds)}")
    if timeout_feeds:
        print(f"  - è¶…æ—¶æº: {[u.split('/')[-1][:25] for u in timeout_feeds[:5]]}")
    if failed_feeds:
        print(f"  - å¤±è´¥æº: {[f[0].split('/')[-1][:25] for f in failed_feeds[:5]]}")

    # Limit articles per source and merge
    # èšç„¦æ¨¡å¼ï¼šæ™ºèƒ½ç¡¬ä»¶æºä¸å—é™åˆ¶ï¼›æ³›AIæ¨¡å¼ï¼šæ‰€æœ‰æºå‡å—é™åˆ¶
    hardware_article_count = 0
    for source, articles in articles_by_source.items():
        # Sort by published time within source
        articles.sort(key=lambda x: x.get("published", ""), reverse=True)

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ™ºèƒ½ç¡¬ä»¶æºï¼ˆé€šè¿‡ feed_url ç²¾ç¡®åŒ¹é…ï¼‰
        is_hardware = hardware_unlimited and any(
            a.get("feed_url", "") in hardware_urls for a in articles
        ) if hardware_urls else False

        if is_hardware:
            # æ™ºèƒ½ç¡¬ä»¶æºï¼šå…¨éƒ¨ä¿ç•™ï¼ˆä»…èšç„¦æ¨¡å¼ï¼‰
            all_articles.extend(articles)
            hardware_article_count += len(articles)
        else:
            # å…¶ä»–æºï¼šæŒ‰ç»„é™åˆ¶æ•°é‡
            feed_url = articles[0].get("feed_url", "") if articles else ""
            group = feed_url_to_group.get(feed_url, "")
            limit = source_limits.get(group, default_limit)
            all_articles.extend(articles[:limit])

    # Sort all by published time (newest first)
    all_articles.sort(key=lambda x: x.get("published", ""), reverse=True)

    print(f"  - Sources with articles: {len(articles_by_source)}")
    if hardware_unlimited:
        print(f"  - Smart hardware articles (unlimited): {hardware_article_count}")
    # Show top sources by article count
    source_counts = [(src, len(arts)) for src, arts in articles_by_source.items()]
    source_counts.sort(key=lambda x: -x[1])
    print(f"  - Top sources: {source_counts[:10]}")

    return all_articles

def apply_filters(articles: list[dict], settings: dict = None) -> list[dict]:
    """Apply blacklist/whitelist filters from settings to articles."""
    if settings is None:
        settings = load_settings()
    filters = settings.get("filters", {})
    blacklist_kw = [kw.lower() for kw in filters.get("blacklist_keywords", [])]
    blacklist_src = [src.lower() for src in filters.get("blacklist_sources", [])]
    whitelist_kw = [kw.lower() for kw in filters.get("whitelist_keywords", [])]
    whitelist_src = [src.lower() for src in filters.get("whitelist_sources", [])]

    if not any([blacklist_kw, blacklist_src, whitelist_kw, whitelist_src]):
        return articles

    filtered = []
    for article in articles:
        title = (article.get("title", "") or "").lower()
        desc = (article.get("description", "") or "").lower()
        source = (article.get("source", "") or "").lower()
        text = title + " " + desc

        # Blacklist: skip if matches
        if any(kw in text for kw in blacklist_kw):
            continue
        if any(src in source for src in blacklist_src):
            continue

        filtered.append(article)

    # Whitelist: boost matching articles to the front
    if whitelist_kw or whitelist_src:
        boosted = []
        normal = []
        for article in filtered:
            title = (article.get("title", "") or "").lower()
            desc = (article.get("description", "") or "").lower()
            source = (article.get("source", "") or "").lower()
            text = title + " " + desc
            if any(kw in text for kw in whitelist_kw) or any(src in source for src in whitelist_src):
                boosted.append(article)
            else:
                normal.append(article)
        filtered = boosted + normal

    return filtered

def get_prompt_for_mode(mode: str, articles_text: str, max_items: int, category_names: str, category_json_example: str, icon_mapping: str, custom_prompt: str = None, paywalled_sources: str = "") -> str:
    """Generate the Claude prompt based on topic mode or custom prompt.

    If custom_prompt is provided, it will be used directly with variable substitution:
    - {articles_text} - The news articles text
    - {max_items} - Maximum number of news items
    - {category_names} - Category names joined by ã€
    - {category_json_example} - Example JSON structure
    - {icon_mapping} - Icon mapping string
    - {paywalled_sources} - Comma-separated list of paywalled source names
    """

    if custom_prompt:
        # Use custom prompt with variable substitution
        try:
            return custom_prompt.format(
                articles_text=articles_text,
                max_items=max_items,
                category_names=category_names,
                category_json_example=category_json_example,
                icon_mapping=icon_mapping,
                paywalled_sources=paywalled_sources
            )
        except KeyError as e:
            print(f"  Warning: Custom prompt has invalid variable {e}, falling back to mode-based prompt")

    if mode == "focused":
        # èšç„¦æ¨¡å¼ï¼šæ‹†æˆä¸¤ä¸ªç‹¬ç«‹ promptï¼Œåˆ†åˆ«è°ƒç”¨ååˆå¹¶
        # è¿”å› Noneï¼Œç”± summarize_news_with_claude å¤„ç†æ‹†åˆ†è°ƒç”¨
        return None

    if mode == "focused_hardware":
        return f"""ä»¥ä¸‹æ˜¯æœ€è¿‘24å°æ—¶å†…ä»å¤šä¸ªæ¥æºæŠ“å–çš„æ–°é—»åˆ—è¡¨ã€‚è¯·ä»ä¸­ç­›é€‰å‡ºä¸**AIæ™ºèƒ½ç¡¬ä»¶è®¾å¤‡**ç›¸å…³çš„æ–°é—»ã€‚

**å±äºæ™ºèƒ½ç¡¬ä»¶çš„èŒƒå›´**ï¼ˆé¢å‘æ¶ˆè´¹è€…æˆ–è¡Œä¸šçš„AIè®¾å¤‡ï¼‰ï¼š
- AR/VR/MR/XR å¤´æ˜¾ã€æ™ºèƒ½çœ¼é•œï¼ˆMeta Ray-Banã€Apple Vision Proã€XREALã€Rokid ç­‰ï¼‰
- AI ç©¿æˆ´è®¾å¤‡ï¼ˆAI æ‰‹è¡¨ã€AI æˆ’æŒ‡ã€AI è€³æœºç­‰ï¼‰
- æœºå™¨äººï¼ˆäººå½¢æœºå™¨äººã€æœåŠ¡æœºå™¨äººã€å·¥ä¸šæœºå™¨äººï¼‰
- AI ç»ˆç«¯è®¾å¤‡ï¼ˆAI æ‰‹æœºã€AI PC ç­‰å…·ä½“è®¾å¤‡äº§å“ï¼‰

**ä¸å±äºæ™ºèƒ½ç¡¬ä»¶**ï¼ˆå¿…é¡»æ’é™¤ï¼Œè¿™äº›åº”æ”¾åœ¨åˆ«çš„ç±»åˆ«ï¼‰ï¼š
- AI èŠ¯ç‰‡ã€AI åŸºç¡€è®¾æ–½ã€æ•°æ®ä¸­å¿ƒæŠ•èµ„ï¼ˆâ†’ å±äºAIæŠ€æœ¯/è¡Œä¸šï¼‰
- å«æ˜Ÿã€èˆªå¤©å™¨ã€å¤ªç©ºè®¾å¤‡ï¼ˆâ†’ å±äºè¡Œä¸šè§‚å¯Ÿï¼‰
- ä¼ ç»Ÿç”µè„‘ã€æ¸¸æˆä¸»æœºï¼ˆPlayStationã€Xboxã€Switchï¼‰
- æ™®é€šæ¶ˆè´¹ç”µå­ï¼ˆç”µè§†ã€éŸ³ç®±ã€ç›¸æœºç­‰ï¼‰
- çº¯è½¯ä»¶äº§å“ã€äº’è”ç½‘æœåŠ¡

**æ•°é‡è¦æ±‚**ï¼šé€‰ 7-10 æ¡ï¼Œä¸å¤šä¸å°‘ã€‚

**ç­›é€‰è¦æ±‚**ï¼š
- å»é‡ï¼šç›¸åŒäº‹ä»¶åªä¿ç•™æœ€æƒå¨æ¥æº
- æŒ‰é‡è¦æ€§æ’åº

**æ¥æºæƒå¨æ€§ä¼˜å…ˆ**ï¼š
- å¦‚æœæŸæ¡æ–°é—»æ¥è‡ªå°ä¼—æ¥æºï¼ˆå¦‚ UploadVR, 93913, VRé™€èº ç­‰ï¼‰ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æƒå¨æ¥æºï¼ˆThe Verge, TechCrunch, Wired ç­‰ï¼‰æŠ¥é“äº†å®Œå…¨ç›¸åŒçš„äº‹ä»¶
- åªæœ‰ç¡®å®šæ˜¯åŒä¸€äº‹ä»¶æ—¶ï¼Œæ‰æ›¿æ¢ä¸ºæƒå¨æ¥æº URL
- âš ï¸ ä½¿ç”¨æŸä¸ªæ¥æºçš„ URL æ—¶ï¼Œæ‘˜è¦å¿…é¡»å‡†ç¡®åæ˜ è¯¥ URL æ–‡ç« çš„å†…å®¹

**ä»˜è´¹å¢™å¤„ç†**ï¼š
ä»˜è´¹å¢™åª’ä½“ï¼š{paywalled_sources}
- å¦‚æœ‰å…è´¹æ›¿ä»£æºæŠ¥é“ç›¸åŒäº‹ä»¶ï¼Œä½¿ç”¨å…è´¹æº URL

**è¾“å‡ºè¦æ±‚**ï¼š
- ä¸ºæ¯æ¡æ–°é—»å†™ä¸€ä¸ªç®€çŸ­çš„ä¸­æ–‡æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰
- ä¸ºæ¯æ¡æ–°é—»æ·»åŠ ä¸€å¥ commentï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªå¯å‘æ€è€ƒçš„é—®é¢˜ï¼ˆä»¥ï¼Ÿç»“å°¾ï¼‰

æ–°é—»åˆ—è¡¨ï¼š
{articles_text}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œç»“æ„å¦‚ä¸‹ï¼š
{{
  "news": [
    {{
      "title": "ä¸­æ–‡æ ‡é¢˜",
      "summary": "1-2å¥ä¸­æ–‡æ‘˜è¦",
      "comment": "ä¸€ä¸ªå¯å‘æ€è€ƒçš„é—®é¢˜ï¼Ÿ",
      "source": "æ¥æº",
      "url": "é“¾æ¥"
    }}
  ]
}}

æ³¨æ„ï¼š
- æ ‡é¢˜å¿…é¡»ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œè‹±æ–‡æ ‡é¢˜ä¸€å¾‹ç¿»è¯‘
- åªè¿”å›åˆæ³•çš„ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
- ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„åŒå¼•å·ç”¨å•å¼•å·æ›¿æ¢"""

    if mode == "focused_ai_industry":
        return f"""ä»¥ä¸‹æ˜¯æœ€è¿‘24å°æ—¶å†…ä»å¤šä¸ªæ¥æºæŠ“å–çš„æ–°é—»åˆ—è¡¨ã€‚è¯·ä»ä¸­ç­›é€‰å‡ºä¸ä»¥ä¸‹ä¸¤ä¸ªåˆ†ç±»ç›¸å…³çš„æ–°é—»ã€‚

**åˆ†ç±» 1ï¼šğŸ¤– AIæŠ€æœ¯ä¸äº§å“**
- æ¨¡å‹èƒ½åŠ›æå‡ï¼šæ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€ã€é•¿ä¸Šä¸‹æ–‡ã€Agent èƒ½åŠ›ç­‰
- æ–°äº§å“å½¢æ€ï¼šAI Agentã€AI ç¼–ç¨‹å·¥å…·ã€AI åˆ›ä½œå·¥å…·ã€AI åº”ç”¨
- æ–°èŒƒå¼ï¼šç«¯ä¾§ AIã€å¼€æºæ¨¡å‹ã€AI åŸºç¡€è®¾æ–½ã€è®­ç»ƒ/æ¨ç†ä¼˜åŒ–

**åˆ†ç±» 2ï¼šğŸ¢ å·¨å¤´åŠ¨å‘ä¸è¡Œä¸šè§‚å¯Ÿ**
- ç§‘æŠ€å·¨å¤´çš„**AIç›¸å…³**æˆ˜ç•¥å¸ƒå±€ã€å¹¶è´­æ”¶è´­
  - å›½å†…å¤§å‚ï¼ˆå­—èŠ‚è·³åŠ¨/è±†åŒ…ã€é˜¿é‡Œ/é€šä¹‰åƒé—®ã€è…¾è®¯ã€ç™¾åº¦/æ–‡å¿ƒã€åä¸ºã€å°ç±³ã€ç¾å›¢ã€äº¬ä¸œç­‰ï¼‰çš„ AI åŠ¨æ€è¦é‡ç‚¹å…³æ³¨
  - æµ·å¤–å·¨å¤´ï¼ˆOpenAIã€Googleã€Metaã€Microsoftã€Appleã€Amazon ç­‰ï¼‰
- AI è¡Œä¸šè¶‹åŠ¿ã€AI æ”¿ç­–æ³•è§„
- AI é¢†åŸŸé‡å¤§æŠ•èèµ„äº‹ä»¶ï¼ˆåŒ…æ‹¬AIèŠ¯ç‰‡ã€AIåŸºç¡€è®¾æ–½æŠ•èµ„ï¼‰
- AI ç›¸å…³é‡è¦äººäº‹å˜åŠ¨

âš ï¸ **ä¸­å¤–æ–°é—»å¹³è¡¡**ï¼šå¦‚æœ‰å›½å†…å¤§å‚ç›¸å…³çš„ AI æ–°é—»ï¼Œè‡³å°‘é€‰å…¥ 1 æ¡ã€‚ä¸è¦å…¨æ˜¯æµ·å¤–æ–°é—»ã€‚

âš ï¸ **æ‰€æœ‰æ–°é—»å¿…é¡»ä¸AI/äººå·¥æ™ºèƒ½ç›´æ¥ç›¸å…³**ã€‚ä»¥ä¸‹ä¸æ”¶å½•ï¼š
- ä¸AIæ— å…³çš„ç§‘æŠ€æ–°é—»ï¼ˆä¼ ç»Ÿåª’ä½“äººäº‹ã€éAIå…¬å¸è£å‘˜ã€åŠ å¯†è´§å¸ç­‰ï¼‰
- çº¯å•†ä¸š/é‡‘èæ–°é—»ï¼ˆé™¤éç›´æ¥æ¶‰åŠAIæŠ•èµ„ï¼‰

**ç­›é€‰è¦æ±‚**ï¼š
- æ’é™¤æ‰€æœ‰æ™ºèƒ½ç¡¬ä»¶è®¾å¤‡æ–°é—»ï¼ˆAR/VRå¤´æ˜¾ã€æ™ºèƒ½çœ¼é•œã€æœºå™¨äººç­‰å…·ä½“è®¾å¤‡ï¼‰
- AIæŠ€æœ¯ä¸äº§å“ï¼šè‡³å°‘é€‰ 2 æ¡
- å·¨å¤´åŠ¨å‘ä¸è¡Œä¸šè§‚å¯Ÿï¼šè‡³å°‘é€‰ 1 æ¡
- ä¸¤ä¸ªåˆ†ç±»åˆè®¡æœ€å¤š {max_items} æ¡
- å»é‡ï¼šç›¸åŒäº‹ä»¶åªä¿ç•™æœ€æƒå¨æ¥æº
- æŒ‰é‡è¦æ€§æ’åº

**æ¥æºæƒå¨æ€§ä¼˜å…ˆ**ï¼š
- ä¸­æ–‡æƒå¨æ¥æºï¼š36æ°ªã€æœºå™¨ä¹‹å¿ƒã€é‡å­ä½ã€è™å—…ã€é’›åª’ä½“ã€æ™šç‚¹LatePostã€Founder Park
- è‹±æ–‡æƒå¨æ¥æºï¼šThe Verge, TechCrunch, Reuters, Bloomberg, Wired
- âš ï¸ ä½¿ç”¨æŸä¸ªæ¥æºçš„ URL æ—¶ï¼Œæ‘˜è¦å¿…é¡»å‡†ç¡®åæ˜ è¯¥ URL æ–‡ç« çš„å†…å®¹

**ä»˜è´¹å¢™å¤„ç†**ï¼š
ä»˜è´¹å¢™åª’ä½“ï¼š{paywalled_sources}
- å¦‚æœ‰å…è´¹æ›¿ä»£æºæŠ¥é“ç›¸åŒäº‹ä»¶ï¼Œä½¿ç”¨å…è´¹æº URL

**è¾“å‡ºè¦æ±‚**ï¼š
- ä¸ºæ¯æ¡æ–°é—»å†™ä¸€ä¸ªç®€çŸ­çš„ä¸­æ–‡æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰
- ä¸ºæ¯æ¡æ–°é—»æ·»åŠ ä¸€å¥ commentï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªå¯å‘æ€è€ƒçš„é—®é¢˜ï¼ˆä»¥ï¼Ÿç»“å°¾ï¼‰

æ–°é—»åˆ—è¡¨ï¼š
{articles_text}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œç»“æ„å¦‚ä¸‹ï¼š
{{
  "categories": [
    {{
      "name": "AIæŠ€æœ¯ä¸äº§å“",
      "icon": "ğŸ¤–",
      "news": [...]
    }},
    {{
      "name": "å·¨å¤´åŠ¨å‘ä¸è¡Œä¸šè§‚å¯Ÿ",
      "icon": "ğŸ¢",
      "news": [...]
    }}
  ]
}}

æ¯æ¡ news çš„ç»“æ„ï¼š
{{
  "title": "ä¸­æ–‡æ ‡é¢˜",
  "summary": "1-2å¥ä¸­æ–‡æ‘˜è¦",
  "comment": "ä¸€ä¸ªå¯å‘æ€è€ƒçš„é—®é¢˜ï¼Ÿ",
  "source": "æ¥æº",
  "url": "é“¾æ¥"
}}

æ³¨æ„ï¼š
- ä¸¤ä¸ªåˆ†ç±»éƒ½å¿…é¡»æœ‰å†…å®¹
- æ ‡é¢˜å¿…é¡»ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œè‹±æ–‡æ ‡é¢˜ä¸€å¾‹ç¿»è¯‘
- åªè¿”å›åˆæ³•çš„ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
- ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„åŒå¼•å·ç”¨å•å¼•å·æ›¿æ¢"""

    else:
        # æ³› AI æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        return f"""ä»¥ä¸‹æ˜¯æœ€è¿‘24å°æ—¶å†…ä»å¤šä¸ªæ¥æºæŠ“å–çš„æ–°é—»åˆ—è¡¨ã€‚è¯·å¸®æˆ‘ï¼š

1. **ä¸¥æ ¼ç­›é€‰**ï¼šåªä¿ç•™ä¸ AIï¼ˆäººå·¥æ™ºèƒ½ï¼‰ç›´æ¥ç›¸å…³çš„æ–°é—»
   - å¿…é¡»åŒ…å«çš„ï¼šAI æ¨¡å‹å‘å¸ƒ/æ›´æ–°ã€AI å…¬å¸åŠ¨æ€ã€AI èèµ„ã€AI äº§å“ã€AI æ”¿ç­–æ³•è§„ã€AI åº”ç”¨è½åœ°ã€å¤§æ¨¡å‹ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€AIGCã€AGIã€æœºå™¨äººã€è‡ªåŠ¨é©¾é©¶ç­‰
   - å¿…é¡»æ’é™¤çš„ï¼šä¸ AI æ— å…³çš„æ™®é€šç§‘æŠ€æ–°é—»ï¼ˆå¦‚æ‰‹æœºå‘å¸ƒã€æ¸¸æˆã€ç”µå•†ä¿ƒé”€ã€ç¤¾äº¤åª’ä½“å…«å¦ã€çº¯ç¡¬ä»¶è¯„æµ‹ç­‰ï¼‰
   - è¾¹ç•Œæƒ…å†µï¼šå¦‚æœä¸€æ¡æ–°é—»ä¸»è¦è®²æŸç§‘æŠ€å…¬å¸ä½†æ ¸å¿ƒå†…å®¹ä¸ AI æ— å…³ï¼Œåº”æ’é™¤
2. å»é‡ï¼šç›¸åŒäº‹ä»¶çš„å¤šç¯‡æŠ¥é“åªä¿ç•™ä¸€æ¡ï¼ˆä¿ç•™æœ€æƒå¨æ¥æºï¼‰
3. **ä¸­å¤–å¹³è¡¡**ï¼šå¦‚æœ‰å›½å†…å¤§å‚ï¼ˆå­—èŠ‚è·³åŠ¨/è±†åŒ…ã€é˜¿é‡Œ/é€šä¹‰åƒé—®ã€è…¾è®¯ã€ç™¾åº¦/æ–‡å¿ƒã€åä¸ºã€å°ç±³ç­‰ï¼‰çš„ AI ç›¸å…³æ–°é—»ï¼Œè‡³å°‘é€‰å…¥ 1 æ¡ï¼Œä¸è¦å…¨æ˜¯æµ·å¤–æ–°é—»
4. æŒ‰é‡è¦æ€§æ’åºï¼ˆå…¨çƒå½±å“ > è¡Œä¸šå½±å“ > åŒºåŸŸå½±å“ï¼‰
4. ä¸ºæ¯æ¡æ–°é—»å†™ä¸€ä¸ªç®€çŸ­çš„ä¸­æ–‡æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰
5. **é‡è¦**ï¼šä¸ºæ¯æ¡æ–°é—»æ·»åŠ ä¸€å¥ commentï¼Œå¿…é¡»æ˜¯ä¸€ä¸ªå¯å‘æ€è€ƒçš„é—®é¢˜ï¼ˆä»¥ï¼Ÿç»“å°¾ï¼‰ï¼Œå¼•å¯¼è¯»è€…æ·±å…¥æ€è€ƒè¿™æ¡æ–°é—»çš„æ„ä¹‰ã€å½±å“æˆ–æœªæ¥å¯èƒ½æ€§
6. å°†æ–°é—»æŒ‰ä»¥ä¸‹ç±»åˆ«åˆ†ç»„ï¼š{category_names}
   - æ¯æ¡æ–°é—»åªå½’å…¥ä¸€ä¸ªæœ€åŒ¹é…çš„ç±»åˆ«
   - æ²¡æœ‰å¯¹åº”æ–°é—»çš„ç±»åˆ«ä¸è¦è¾“å‡º

**é‡è¦**ï¼šæ€»å…±æœ€å¤šé€‰ {max_items} æ¡æœ€å€¼å¾—çœ‹çš„æ–°é—»ï¼ˆä¸æ˜¯æ¯ä¸ªåˆ†ç±» {max_items} æ¡ï¼‰ï¼Œåœ¨è¿™äº›æ–°é—»ä¸­å½’ç±»æ’åˆ—ã€‚
æ‘˜è¦å’Œæ ‡é¢˜ä¸­ä¸è¦ä½¿ç”¨åŒå¼•å·ï¼Œç”¨å•å¼•å·æˆ–å…¶ä»–æ ‡ç‚¹ä»£æ›¿ã€‚

æ–°é—»åˆ—è¡¨ï¼š
{articles_text}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼Œç»“æ„å¦‚ä¸‹ï¼š
{{
  "categories": {category_json_example}
}}

æ³¨æ„ï¼š
- åªè¿”å›æœ‰æ–°é—»çš„ç±»åˆ«
- icon å¿…é¡»ä¸ç±»åˆ«å¯¹åº”ï¼ˆ{icon_mapping}ï¼‰
- æ¯æ¡ news å¿…é¡»åŒ…å« comment å­—æ®µï¼ˆå¯å‘æ€è€ƒçš„é—®å¥ï¼Œä»¥ï¼Ÿç»“å°¾ï¼‰
- åªè¿”å›åˆæ³•çš„ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
- ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„åŒå¼•å·ç”¨å•å¼•å·æ›¿æ¢"""


def _call_deepseek(prompt: str, label: str) -> str:
    """Call DeepSeek V3 API and return response text. Returns None on failure."""
    import time
    import re
    from openai import OpenAI as OpenAIClient

    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        return None

    client = OpenAIClient(api_key=api_key, base_url="https://api.deepseek.com")
    start = time.time()
    try:
        resp = client.chat.completions.create(
            model="deepseek-chat",
            max_tokens=8192,
            messages=[{"role": "user", "content": prompt}],
        )
        elapsed = time.time() - start
        print(f"  - DeepSeek ({label}) è€—æ—¶: {elapsed:.1f}s")
        text = resp.choices[0].message.content
        # Strip <think> tags if present
        text = re.sub(r'<think>[\s\S]*?</think>', '', text).strip()
        return text
    except Exception as e:
        print(f"  - DeepSeek ({label}) error: {e}")
        return None


def _parse_json_response(response_text: str):
    """Extract and parse JSON from model response text. Returns parsed dict or None.

    Uses multi-pass JSON repair matching the main summarize function.
    """
    import re
    start_idx = response_text.find('{')
    end_idx = response_text.rfind('}') + 1
    if start_idx == -1 or end_idx <= start_idx:
        return None
    json_str = response_text[start_idx:end_idx]

    # Pass 1: direct parse
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as first_error:
        print(f"  - JSON parse error (attempting fix): {first_error}")

    # Pass 2: control chars + unescaped quotes fix
    json_str = re.sub(r'[\x00-\x1f\x7f]', ' ', json_str)

    def fix_quotes_in_value(match):
        key = match.group(1)
        value = match.group(2)
        fixed_value = value.replace('"', "'")
        return f'"{key}": "{fixed_value}"'

    json_str = re.sub(
        r'"(title|summary|comment|source|url|name|icon)"\s*:\s*"((?:[^"\\]|\\.)*)(?<!\\)"',
        fix_quotes_in_value,
        json_str
    )

    try:
        return json.loads(json_str)
    except json.JSONDecodeError as second_error:
        print(f"  - JSON fix attempt 1 failed: {second_error}")

    # Pass 3: trailing commas + extract categories array
    json_str = re.sub(r',\s*}', '}', json_str)
    json_str = re.sub(r',\s*]', ']', json_str)

    try:
        return json.loads(json_str)
    except json.JSONDecodeError:
        pass

    try:
        cat_match = re.search(r'"categories"\s*:\s*(\[[\s\S]*\])', json_str)
        if cat_match:
            categories_str = cat_match.group(1)
            categories_str = re.sub(r',\s*}', '}', categories_str)
            categories_str = re.sub(r',\s*]', ']', categories_str)
            result = json.loads(categories_str)
            print(f"  - Recovered {len(result)} categories from partial JSON")
            return {"categories": result}
    except Exception:
        pass

    # Pass 4: line-by-line quote reconstruction
    try:
        lines = json_str.split('\n')
        fixed_lines = []
        for line in lines:
            m = re.match(r'^(\s*"(?:title|summary|comment|source|url|name|icon)":\s*")(.*)(",?\s*)$', line)
            if m:
                value = m.group(2).replace('"', "'")
                line = m.group(1) + value + m.group(3)
            fixed_lines.append(line)
        json_str = '\n'.join(fixed_lines)
        return json.loads(json_str)
    except Exception as final_error:
        print(f"  - All JSON fix attempts failed: {final_error}")
        return None


def _call_haiku(client, prompt: str, label: str) -> str:
    """Call Claude Haiku and return response text. Returns None on failure."""
    import time
    try:
        start = time.time()
        resp = client.messages.create(
            model="claude-haiku-4-5-20251001",
            max_tokens=8192,
            messages=[{"role": "user", "content": prompt}],
        )
        elapsed = time.time() - start
        print(f"  - Haiku ({label}) {elapsed:.1f}s, stop_reason: {resp.stop_reason}")
        if resp.stop_reason == "max_tokens":
            print(f"  - WARNING: Response was truncated (hit max_tokens)")
        return resp.content[0].text
    except Exception as e:
        print(f"  - Haiku ({label}) error: {e}")
        return None


def _call_ai(prompt: str, label: str, anthropic_client=None) -> str:
    """Call the best available AI backend. Tries DeepSeek first, falls back to Haiku.

    Returns response text, or None if all backends fail.
    """
    if os.environ.get("DEEPSEEK_API_KEY"):
        result = _call_deepseek(prompt, label)
        if result:
            return result
        print(f"  - DeepSeek failed for {label}, trying Haiku fallback...")
    if anthropic_client:
        return _call_haiku(anthropic_client, prompt, label)
    print(f"  - No AI backend available for {label}")
    return None


def _format_articles_text(articles: list[dict]) -> str:
    """Format a list of article dicts into text for Claude prompts."""
    text = ""
    for i, article in enumerate(articles, 1):
        text += f"""
---
Article {i}:
Title: {article.get('title', '')}
Source: {article.get('source', '')}
Published: {article.get('published', '')}
Description: {article.get('description', '')}
URL: {article.get('url', '')}
"""
    return text


def _focused_split_call(client, articles: list[dict], max_items: int, paywalled_sources: str, settings: dict) -> list[dict]:
    """Focused mode: two sequential AI calls for hardware and AI/industry, then merge."""
    import time

    print(f"  - Focused mode: 2 AI calls (hardware + AI/industry)")

    # Split articles: hardware sources vs others
    hw_urls = set()
    for feed in settings.get("rss_feeds", []):
        if feed.get("group") == "æ™ºèƒ½ç¡¬ä»¶" and feed.get("enabled", True):
            hw_urls.add(feed.get("url", ""))

    hw_articles = [a for a in articles if a.get("feed_url", "") in hw_urls]
    other_articles = [a for a in articles if a.get("feed_url", "") not in hw_urls]
    print(f"  - Article split: {len(hw_articles)} hardware, {len(other_articles)} other")

    other_articles_text = _format_articles_text(other_articles) if other_articles else _format_articles_text(articles)

    if hw_articles:
        hw_articles_text = _format_articles_text(hw_articles)
        hw_budget = 10  # hardware gets 7-10 items
        ai_budget = max(max_items - hw_budget, 5)  # rest goes to AI+industry, at least 5
        prompt_hw = get_prompt_for_mode("focused_hardware", hw_articles_text, max_items, "", "", "", None, paywalled_sources)
        print(f"  - Budget: hardware 7-10, AI+industry {ai_budget}, total cap {max_items}")
    else:
        # No hardware sources produced articles -- skip hardware prompt, give all budget to AI/industry
        print("  - No hardware articles found, skipping hardware prompt")
        hw_budget = 0
        ai_budget = max_items
        prompt_hw = None

    prompt_ai = get_prompt_for_mode("focused_ai_industry", other_articles_text, ai_budget, "", "", "", None, paywalled_sources)

    start = time.time()

    def _call_and_parse(prompt, label, max_retries=2):
        """Call AI and parse JSON, with retries for both API and parse failures."""
        for attempt in range(max_retries + 1):
            if attempt > 0:
                print(f"  - Retrying {label} (attempt {attempt + 1})...")
                time.sleep(3)
            resp = _call_ai(prompt, f"{label}" if attempt == 0 else f"{label}-retry{attempt}", anthropic_client=client)
            if not resp:
                print(f"  - {label}: API call returned None")
                continue
            parsed = _parse_json_response(resp)
            if parsed:
                return parsed
            print(f"  - {label}: JSON parse failed. Preview: {resp[:200]}")
        return None

    hw_parsed = _call_and_parse(prompt_hw, "æ™ºèƒ½ç¡¬ä»¶") if prompt_hw else None
    ai_parsed = _call_and_parse(prompt_ai, "AI+è¡Œä¸š")

    elapsed = time.time() - start
    print(f"  - Focused split total è€—æ—¶: {elapsed:.1f}s")

    categories = []

    # Parse hardware result
    if hw_parsed:
        hw_news = hw_parsed.get("news", [])
        if hw_news:
            categories.append({"name": "æ™ºèƒ½ç¡¬ä»¶", "icon": "ğŸ¥½", "news": hw_news})
            print(f"  - ğŸ¥½ æ™ºèƒ½ç¡¬ä»¶: {len(hw_news)} æ¡")
        else:
            print(f"  - ğŸ¥½ æ™ºèƒ½ç¡¬ä»¶: parsed OK but no 'news' key. Keys: {list(hw_parsed.keys())}")
    else:
        print(f"  - ğŸ¥½ æ™ºèƒ½ç¡¬ä»¶: all attempts failed")

    # Collect URLs from hardware for dedup
    seen_urls = set()
    for cat in categories:
        for news in cat.get("news", []):
            url = news.get("url", "")
            if url:
                seen_urls.add(url)

    if ai_parsed:
        ai_cats = ai_parsed.get("categories", [])
        print(f"  - AI+è¡Œä¸š: parsed OK, {len(ai_cats)} categories. Keys: {list(ai_parsed.keys())}")
        for cat in ai_cats:
            cat_name = cat.get("name", "?")
            cat_news = cat.get("news", [])
            deduped_news = [n for n in cat_news if n.get("url", "") not in seen_urls]
            removed = len(cat_news) - len(deduped_news)
            if deduped_news:
                categories.append({"name": cat_name, "icon": cat.get("icon", ""), "news": deduped_news})
                msg = f"  - {cat.get('icon', '')} {cat_name}: {len(deduped_news)} æ¡"
                if removed > 0:
                    msg += f" (å»é‡ç§»é™¤ {removed} æ¡)"
                print(msg)
            else:
                print(f"  - {cat.get('icon', '')} {cat_name}: 0 æ¡ (all {len(cat_news)} deduped)")
    else:
        print(f"  - AI+è¡Œä¸š: all attempts failed")

    total = sum(len(c.get("news", [])) for c in categories)
    print(f"  - Focused total: {total} items in {len(categories)} categories")

    if not categories:
        print(f"  - WARNING: Both calls failed, returning empty")

    return categories


def summarize_news_with_claude(anthropic_key: str, articles: list[dict], max_items: int = 10, settings: dict = None) -> list[dict]:
    """Use AI to summarize, categorize, and select top news.

    Tries DeepSeek first, falls back to Claude Haiku if DeepSeek is unavailable.
    """

    if not articles:
        return []

    if settings is None:
        settings = load_settings()

    topic_mode = settings.get("topic_mode", "broad")  # "broad" or "focused"
    custom_prompt = settings.get("custom_prompt", "")  # User-defined custom prompt
    client = anthropic.Anthropic(api_key=anthropic_key) if (anthropic_key and anthropic) else None

    # èšç„¦æ¨¡å¼ä½¿ç”¨ä¸“é—¨çš„ 3 ä¸ªåˆ†ç±»
    if topic_mode == "focused" and not custom_prompt:
        categories = [
            {"name": "æ™ºèƒ½ç¡¬ä»¶", "icon": "ğŸ¥½"},
            {"name": "AIæŠ€æœ¯ä¸äº§å“", "icon": "ğŸ¤–"},
            {"name": "å·¨å¤´åŠ¨å‘ä¸è¡Œä¸šè§‚å¯Ÿ", "icon": "ğŸ¢"},
        ]
    else:
        categories = get_categories(settings)

    if custom_prompt:
        print(f"  - Using custom prompt ({len(custom_prompt)} chars)")
    else:
        print(f"  - Topic mode: {topic_mode}")

    # Prepare articles for Claude
    articles_text = ""
    for i, article in enumerate(articles[:120], 1):  # Limit to 120 articles for diversity
        articles_text += f"""
---
Article {i}:
Title: {article.get('title', '')}
Source: {article.get('source', '')}
Published: {article.get('published', '')}
Description: {article.get('description', '')}
URL: {article.get('url', '')}
"""

    category_names = "ã€".join(c["name"] for c in categories)
    category_json_example = json.dumps(
        [{"name": c["name"], "icon": c["icon"], "news": [{"title": "...", "summary": "...", "comment": "ä¸€ä¸ªå¯å‘æ€è€ƒçš„é—®é¢˜ï¼Ÿ", "source": "...", "url": "..."}]} for c in categories[:2]],
        ensure_ascii=False, indent=4
    )

    icon_mapping = " ".join(f'{c["name"]}:{c["icon"]}' for c in categories)

    # è·å–ä»˜è´¹å¢™æºåç§°
    rss_feeds = settings.get("rss_feeds", [])
    paywalled_sources = ", ".join(
        feed.get("name", "") for feed in rss_feeds
        if feed.get("paywalled", False) and feed.get("enabled", True)
    )
    if paywalled_sources:
        print(f"  - Paywalled sources: {paywalled_sources}")

    prompt = get_prompt_for_mode(topic_mode, articles_text, max_items, category_names, category_json_example, icon_mapping, custom_prompt, paywalled_sources)

    import time
    claude_start = time.time()

    # Focused mode: split into 2 parallel calls (hardware + AI/industry)
    if prompt is None and topic_mode == "focused":
        return _focused_split_call(client, articles[:120], max_items, paywalled_sources, settings)

    response_text = _call_ai(prompt, topic_mode, anthropic_client=client)
    if not response_text:
        print(f"  Error: All AI backends failed for {topic_mode} mode")
        return []

    claude_elapsed = time.time() - claude_start
    print(f"  - AI ({topic_mode}) è€—æ—¶: {claude_elapsed:.1f}s")

    parsed = _parse_json_response(response_text)
    if parsed:
        return parsed.get("categories", [])

    print(f"  Error: Failed to parse AI response")
    return []

def fetch_news(anthropic_key: str = "", topic: str = "AI/ç§‘æŠ€", max_items: int = 10, settings: dict = None, manual: bool = False, hardware_unlimited: bool = None, channel: dict = None) -> dict:
    """Fetch and process news.

    Args:
        anthropic_key: API key for Claude (optional if DEEPSEEK_API_KEY is set)
        topic: News topic
        max_items: Maximum news items to return
        settings: Configuration dict
        manual: If True, use current time as window end (manual trigger)
        hardware_unlimited: Override for hardware source limiting. If None, auto-detect from topic_mode.
        channel: Optional channel dict for time window calculation.

    Returns dict with categories and _raw_articles (for multi-channel reuse).
    """

    if settings is None:
        settings = load_settings()

    tz_name = settings.get("timezone", "Asia/Shanghai")
    tz = ZoneInfo(tz_name)
    today = datetime.now(tz).strftime("%Y-%m-%d")
    start_time, end_time = get_time_window(settings, manual=manual, channel=channel)
    cutoff = get_cutoff_time(settings, manual=manual, channel=channel)

    print(f"  - Time window: {start_time} ~ {end_time}")

    # èšç„¦æ¨¡å¼ä¸‹ï¼Œæ™ºèƒ½ç¡¬ä»¶æºä¸å—æ•°é‡é™åˆ¶
    if hardware_unlimited is None:
        topic_mode = settings.get("topic_mode", "broad")
        hardware_unlimited = (topic_mode == "focused")

    print("  - Fetching news from RSS feeds...")
    raw_articles = fetch_raw_news(cutoff=cutoff, settings=settings, hardware_unlimited=hardware_unlimited)
    print(f"  - Got {len(raw_articles)} raw articles")

    # Apply blacklist/whitelist filters
    raw_articles = apply_filters(raw_articles, settings)
    print(f"  - After filtering: {len(raw_articles)} articles")

    if not raw_articles:
        return {
            "date": today,
            "time_window": f"{start_time} ~ {end_time}",
            "categories": [],
            "_raw_articles": [],
            "error": "No articles fetched from RSS feeds"
        }

    backend = "DeepSeek" if os.environ.get("DEEPSEEK_API_KEY") else "Claude"
    print(f"  - Summarizing with {backend}...")
    categories = summarize_news_with_claude(anthropic_key, raw_articles, max_items, settings)
    total = sum(len(c.get("news", [])) for c in categories)
    print(f"  - Selected {total} top news in {len(categories)} categories")

    return {
        "date": today,
        "time_window": f"{start_time} ~ {end_time}",
        "categories": categories,
        "_raw_articles": raw_articles,
    }

def save_draft(news_data: dict, settings: dict = None, channel_id: str = None) -> str:
    """Save news data as a draft JSON file.

    Args:
        news_data: The news data dict (categories, date, etc.)
        settings: Configuration dict
        channel_id: If set, saves as a channel-specific draft (YYYY-MM-DD_ch_<id>.json)

    Returns the draft file path.
    """
    if settings is None:
        settings = load_settings()

    date = news_data.get("date", datetime.now().strftime("%Y-%m-%d"))
    drafts_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config", "drafts")
    os.makedirs(drafts_dir, exist_ok=True)

    if channel_id:
        filename = f"{date}_ch_{channel_id}.json"
    else:
        filename = f"{date}.json"
    draft_path = os.path.join(drafts_dir, filename)

    # Never overwrite a draft that's already been sent or rejected
    if os.path.exists(draft_path):
        try:
            with open(draft_path, "r", encoding="utf-8") as f:
                existing = json.load(f)
            existing_status = existing.get("status", "")
            if existing_status in ("sent", "rejected", "approved"):
                print(f"  - Skipping {filename}: already {existing_status}")
                return draft_path
        except (json.JSONDecodeError, IOError):
            pass  # Corrupted file, safe to overwrite

    # Filter out internal fields like _raw_articles
    clean_data = {k: v for k, v in news_data.items() if not k.startswith("_")}

    draft_data = {
        **clean_data,
        "status": news_data.get("status", "pending_review"),
        "source": news_data.get("source", "scheduled"),
        "created_at": datetime.now(ZoneInfo(settings.get("timezone", "Asia/Shanghai"))).isoformat(),
    }

    # Add channel metadata for channel drafts
    if channel_id:
        draft_data["channel_id"] = channel_id
        # Find channel config to store name and topic_mode
        all_channels = settings.get("channels", settings.get("webhook_channels", []))
        for ch in all_channels:
            if ch.get("id") == channel_id:
                draft_data["channel_name"] = ch.get("name", "")
                draft_data["topic_mode"] = ch.get("topic_mode", "broad")
                break

    with open(draft_path, "w", encoding="utf-8") as f:
        json.dump(draft_data, f, ensure_ascii=False, indent=2)

    print(f"  - Draft saved to {draft_path}")

    # æ¸…ç† 30 å¤©å‰çš„æ—§è‰ç¨¿
    cleanup_old_drafts(drafts_dir, days=30)

    return draft_path


def cleanup_old_drafts(drafts_dir: str, days: int = 30):
    """Delete draft files older than specified days.

    Handles both YYYY-MM-DD.json and YYYY-MM-DD_ch_<id>.json formats.
    """
    cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    deleted = []

    try:
        for filename in os.listdir(drafts_dir):
            if not filename.endswith('.json'):
                continue
            # Extract date from filename: YYYY-MM-DD.json or YYYY-MM-DD_ch_xxx.json
            base = filename.replace('.json', '')
            # Date is always the first 10 chars (YYYY-MM-DD)
            file_date = base[:10]
            if len(file_date) == 10 and file_date < cutoff_date:
                filepath = os.path.join(drafts_dir, filename)
                os.remove(filepath)
                deleted.append(filename)
    except Exception as e:
        print(f"  Warning: Failed to cleanup old drafts: {e}")

    if deleted:
        print(f"  - Cleaned up {len(deleted)} old drafts: {deleted}")

def load_draft(date: str = None, channel_id: str = None):
    """Load a draft by date and optional channel_id.

    Args:
        date: Date string (YYYY-MM-DD). Defaults to today.
        channel_id: If set, loads the channel-specific draft.

    Returns the draft data dict, or None if not found.
    """
    if date is None:
        settings = load_settings()
        tz = ZoneInfo(settings.get("timezone", "Asia/Shanghai"))
        date = datetime.now(tz).strftime("%Y-%m-%d")

    drafts_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "config", "drafts")

    if channel_id:
        filename = f"{date}_ch_{channel_id}.json"
    else:
        filename = f"{date}.json"
    draft_path = os.path.join(drafts_dir, filename)

    try:
        with open(draft_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return None

def format_email_html(news_data: dict, settings: dict = None) -> str:
    """Format news data into a beautiful HTML email.

    Categories are rendered in the order from the draft (èšç„¦æ¨¡å¼çš„é¡ºåºç”± Claude è¿”å›).
    """
    if settings is None:
        settings = load_settings()

    date = news_data.get("date", "")
    time_window = news_data.get("time_window", "")
    raw_categories = news_data.get("categories", [])

    # Build category sections - ç›´æ¥æŒ‰è‰ç¨¿ä¸­çš„é¡ºåºæ˜¾ç¤º
    sections_html = ""
    has_news = False
    for cat in raw_categories:
        cat_name = cat.get("name", "")
        news_items = cat.get("news", [])
        if not news_items:
            continue
        has_news = True
        icon = CATEGORY_ICONS.get(cat_name, cat.get("icon", "ğŸ“°"))

        cards_html = ""
        for item in news_items:
            title = item.get("title", "")
            summary = item.get("summary", "")
            comment = item.get("comment", "")
            source = item.get("source", "")
            url = item.get("url", "#")

            comment_html = ""
            if comment:
                comment_html = f'<p style="color:#7c3aed;font-size:13px;line-height:1.5;margin:8px 0 10px 0;padding:8px 12px;background:#f5f3ff;border-radius:6px;">ğŸ¤” {comment}</p>'

            cards_html += f'''<table width="100%" cellpadding="0" cellspacing="0" border="0" style="margin-bottom:12px;">
<tr><td style="background:#ffffff;border-radius:8px;border:1px solid #e8e8e8;padding:16px 20px;">
  <a href="{url}" style="color:#1a1a2e;text-decoration:none;font-size:15px;font-weight:600;line-height:1.4;display:block;" target="_blank">{title}</a>
  <p style="color:#555;font-size:14px;line-height:1.6;margin:8px 0 10px 0;">{summary}</p>
  {comment_html}
  <span style="display:inline-block;background:#eef2ff;color:#4f46e5;font-size:12px;padding:2px 10px;border-radius:12px;">{source}</span>
</td></tr>
</table>'''

        sections_html += f'''<tr><td style="padding:24px 30px 8px 30px;">
  <h2 style="margin:0 0 16px 0;font-size:18px;color:#1a1a2e;font-weight:700;">{icon} {cat_name}</h2>
  {cards_html}
</td></tr>'''

    if not has_news:
        sections_html = '<tr><td style="padding:20px 30px;color:#666;font-size:16px;">ä»Šæ—¥æš‚æ— é‡è¦æ–°é—»ã€‚</td></tr>'

    html = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"></head>
<body style="margin:0;padding:0;background-color:#f0f2f5;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI','PingFang SC','Hiragino Sans GB','Microsoft YaHei',sans-serif;">
<table width="100%" cellpadding="0" cellspacing="0" border="0" style="background-color:#f0f2f5;">
<tr><td align="center" style="padding:24px 16px;">
<table width="100%" cellpadding="0" cellspacing="0" border="0" style="max-width:640px;background-color:#ffffff;border-radius:12px;overflow:hidden;box-shadow:0 2px 12px rgba(0,0,0,0.08);">

<!-- Header -->
<tr><td style="background-color:#1a1a2e;background:linear-gradient(135deg,#1a1a2e 0%,#16213e 50%,#0f3460 100%);padding:32px 30px;text-align:center;">
  <h1 style="margin:0;color:#ffffff;font-size:22px;font-weight:700;letter-spacing:1px;">AI / ç§‘æŠ€æ–°é—»æ—¥æŠ¥</h1>
  <p style="margin:10px 0 0 0;color:rgba(255,255,255,0.75);font-size:14px;">{date} &nbsp;|&nbsp; {time_window}</p>
</td></tr>

<!-- News Sections -->
{sections_html}

<!-- Footer -->
<tr><td style="padding:20px 30px;border-top:1px solid #eee;text-align:center;">
  <p style="margin:0;color:#999;font-size:12px;">ç”± AI News Assistant è‡ªåŠ¨ç”Ÿæˆ &nbsp;&middot;&nbsp; Powered by Claude</p>
</td></tr>

</table>
</td></tr>
</table>
</body>
</html>'''

    return html

if __name__ == "__main__":
    anthropic_key = os.environ.get("ANTHROPIC_API_KEY", "")
    deepseek_key = os.environ.get("DEEPSEEK_API_KEY", "")

    if not anthropic_key and not deepseek_key:
        print("Error: Neither ANTHROPIC_API_KEY nor DEEPSEEK_API_KEY is set")
        exit(1)

    news_data = fetch_news(anthropic_key)
    print(json.dumps(news_data, ensure_ascii=False, indent=2))
